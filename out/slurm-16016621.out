
Due to MODULEPATH changes, the following have been reloaded:
  1) libfabric/1.10.1     2) openmpi/4.0.3     3) ucx/1.8.0


Lmod is automatically replacing "intel/2020.1.217" with "gcc/9.3.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) blis/0.8.1     2) cuda/11.4     3) flexiblas/3.0.4     4) openmpi/4.0.3

Tue Apr 18 15:11:55 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  Off  | 00000000:01:00.0 Off |                    0 |
| N/A   31C    P0    54W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM...  Off  | 00000000:41:00.0 Off |                    0 |
| N/A   29C    P0    51W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A100-SXM...  Off  | 00000000:81:00.0 Off |                    0 |
| N/A   31C    P0    53W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A100-SXM...  Off  | 00000000:C1:00.0 Off |                    0 |
| N/A   30C    P0    54W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Testing...
23-04-18 15:12:01.271 :   task: ITMO TRAINING
  model: plain
  gpu_ids: [0, 1, 2, 3]
  dist: False
  scale: 1
  n_channels: 3
  path:[
    root: SwinITMO
    pretrained_netG: SwinITMO/ITMO TRAINING/models/600_G.pth
    pretrained_netE: SwinITMO/ITMO TRAINING/models/600_E.pth
    task: SwinITMO/ITMO TRAINING
    log: SwinITMO/ITMO TRAINING
    options: SwinITMO/ITMO TRAINING/options
    models: SwinITMO/ITMO TRAINING/models
    images: SwinITMO/ITMO TRAINING/images
    pretrained_optimizerG: SwinITMO/ITMO TRAINING/models/600_optimizerG.pth
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: itmo
      dataroot: /home/slee67/KAIR/train_itmo_dirs.txt
      H_size: 64
      dataloader_shuffle: True
      dataloader_num_workers: 16
      dataloader_batch_size: 8
      phase: train
      scale: 1
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: itmo
      dataroot: /home/slee67/KAIR/test_itmo_dirs.txt
      phase: test
      scale: 1
      n_channels: 3
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 1
    in_chans: 3
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: null
    resi_connection: 1conv
    init_type: default
    scale: 1
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 1000000
    checkpoint_save: 100
    checkpoint_print: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: /home/slee67/KAIR/options/swinir/train_swinir_hdr.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  use_static_graph: False
  num_gpu: 4
  rank: 0
  world_size: 1

23-04-18 15:12:01.394 : Number of train images: 15,449, iters: 1,932
/home/slee67/ENV/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/slee67/ENV/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /home/coulombc/wheels_builder/tmp.17380/python-3.10/torch/aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
23-04-18 15:12:06.126 : 
Networks name: SwinIR
Params number: 11504163
Net structure:
SwinIR(
  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.003)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.006)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.009)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.011)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.014)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.017)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.020)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.023)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.026)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.029)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.031)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.034)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.037)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.040)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.043)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.046)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.049)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.051)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.054)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.057)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.060)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.063)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.066)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.069)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.071)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.074)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.077)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.080)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.083)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.086)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.089)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.091)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.094)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.097)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.100)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_last): Conv2d(180, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

23-04-18 15:12:06.212 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.001 | -0.200 |  0.199 |  0.110 | torch.Size([180, 3, 3, 3]) || conv_first.weight
 |  0.010 | -0.192 |  0.192 |  0.113 | torch.Size([180]) || conv_first.bias
 |  1.001 |  0.997 |  1.006 |  0.002 | torch.Size([180]) || patch_embed.norm.weight
 | -0.000 | -0.004 |  0.004 |  0.002 | torch.Size([180]) || patch_embed.norm.bias
 |  0.998 |  0.992 |  1.004 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 | -0.006 |  0.008 |  0.003 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.057 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.096 |  0.083 |  0.021 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 | -0.000 | -0.027 |  0.021 |  0.005 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.086 |  0.076 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight
 | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.002 |  0.994 |  1.014 |  0.004 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight
 | -0.000 | -0.008 |  0.008 |  0.003 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.091 |  0.092 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 | -0.000 | -0.009 |  0.007 |  0.002 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.086 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  0.999 |  0.992 |  1.003 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.064 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.102 |  0.095 |  0.021 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.023 |  0.028 |  0.005 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.080 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight
 | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.001 |  0.995 |  1.014 |  0.003 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.007 |  0.006 |  0.003 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.099 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 | -0.000 | -0.009 |  0.007 |  0.002 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.093 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  0.999 |  0.993 |  1.004 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.007 |  0.006 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.074 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.096 |  0.101 |  0.021 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 | -0.000 | -0.028 |  0.034 |  0.007 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.082 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight
 | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.001 |  0.994 |  1.021 |  0.003 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.008 |  0.006 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.095 |  0.094 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 | -0.000 | -0.007 |  0.008 |  0.002 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.091 |  0.094 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  0.999 |  0.993 |  1.005 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.061 |  0.075 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.085 |  0.095 |  0.021 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.027 |  0.023 |  0.007 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.088 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight
 | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  0.995 |  1.012 |  0.003 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.083 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 | -0.001 | -0.011 |  0.005 |  0.002 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.080 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  0.999 |  0.994 |  1.003 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.004 |  0.005 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.061 |  0.073 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.101 |  0.095 |  0.021 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight
 |  0.000 | -0.030 |  0.027 |  0.008 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.081 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias
 |  0.999 |  0.994 |  1.008 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.092 |  0.078 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight
 | -0.000 | -0.009 |  0.010 |  0.002 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.084 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight
 | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask
 |  0.999 |  0.992 |  1.004 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight
 | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.059 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.090 |  0.097 |  0.021 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.024 |  0.024 |  0.006 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.076 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight
 | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  0.994 |  1.008 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.007 |  0.007 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.083 |  0.097 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight
 | -0.000 | -0.008 |  0.007 |  0.002 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.083 |  0.089 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.031 |  0.031 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight
 | -0.003 | -0.027 |  0.026 |  0.014 | torch.Size([180]) || layers.0.conv.bias
 |  0.999 |  0.995 |  1.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.075 |  0.067 |  0.019 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.088 |  0.095 |  0.021 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 | -0.001 | -0.023 |  0.022 |  0.005 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.074 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight
 | -0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  0.995 |  1.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight
 | -0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.085 |  0.095 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 | -0.000 | -0.010 |  0.006 |  0.002 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.082 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  0.995 |  1.004 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight
 | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.073 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.087 |  0.092 |  0.021 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.021 |  0.024 |  0.006 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.080 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight
 | -0.000 | -0.005 |  0.006 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  0.995 |  1.007 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.085 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 | -0.000 | -0.008 |  0.004 |  0.002 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.083 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  0.999 |  0.995 |  1.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.057 |  0.067 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.094 |  0.096 |  0.021 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 | -0.000 | -0.023 |  0.023 |  0.006 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.086 |  0.075 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight
 | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  0.992 |  1.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 | -0.007 |  0.008 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.090 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 | -0.000 | -0.008 |  0.007 |  0.002 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.091 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 | -0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  0.994 |  1.007 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 | -0.005 |  0.007 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.073 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.094 |  0.091 |  0.021 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.018 |  0.018 |  0.004 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.089 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight
 | -0.000 | -0.005 |  0.006 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  0.995 |  1.006 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 | -0.004 |  0.006 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.090 |  0.091 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.077 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.999 |  0.992 |  1.003 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight
 | -0.000 | -0.007 |  0.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.066 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.091 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.015 |  0.010 |  0.003 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.082 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  0.991 |  1.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.086 |  0.092 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight
 | -0.000 | -0.007 |  0.005 |  0.002 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.086 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask
 |  0.999 |  0.996 |  1.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight
 | -0.000 | -0.006 |  0.004 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.064 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.089 |  0.092 |  0.021 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight
 | -0.000 | -0.015 |  0.013 |  0.003 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.084 |  0.088 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight
 | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  0.989 |  1.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight
 |  0.000 | -0.007 |  0.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.091 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight
 | -0.000 | -0.006 |  0.006 |  0.002 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.085 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.006 |  0.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.032 |  0.034 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight
 |  0.001 | -0.026 |  0.026 |  0.014 | torch.Size([180]) || layers.1.conv.bias
 |  1.000 |  0.995 |  1.006 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.006 |  0.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.063 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.081 |  0.096 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.080 |  0.090 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight
 | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  0.996 |  1.006 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight
 | -0.000 | -0.004 |  0.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.086 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 | -0.000 | -0.006 |  0.005 |  0.002 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.082 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  0.996 |  1.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.005 |  0.006 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.068 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.089 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.004 |  0.004 |  0.001 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.079 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight
 | -0.000 | -0.004 |  0.004 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  0.994 |  1.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.085 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 | -0.000 | -0.006 |  0.005 |  0.002 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.078 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.996 |  1.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.007 |  0.007 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.069 |  0.073 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.092 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.009 |  0.006 |  0.002 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.081 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight
 | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  0.996 |  1.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.083 |  0.097 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 | -0.000 | -0.004 |  0.005 |  0.002 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.081 |  0.078 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  0.995 |  1.007 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.007 |  0.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.064 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.087 |  0.081 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.007 |  0.008 |  0.002 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.089 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight
 | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  0.996 |  1.007 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.074 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 | -0.000 | -0.007 |  0.004 |  0.002 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.092 |  0.094 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.006 |  0.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  0.996 |  1.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.068 |  0.065 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.080 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 | -0.006 |  0.005 |  0.001 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.087 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  0.996 |  1.006 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.006 |  0.004 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.089 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight
 | -0.000 | -0.005 |  0.006 |  0.002 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.090 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight
 | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  0.996 |  1.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.075 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.096 |  0.093 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.009 |  0.008 |  0.002 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.081 |  0.090 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight
 | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  0.994 |  1.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.004 |  0.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.093 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight
 | -0.000 | -0.007 |  0.006 |  0.002 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.085 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.029 |  0.032 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight
 |  0.001 | -0.025 |  0.028 |  0.014 | torch.Size([180]) || layers.2.conv.bias
 |  1.000 |  0.995 |  1.006 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 | -0.007 |  0.005 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.069 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.086 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.007 |  0.007 |  0.002 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.080 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  0.995 |  1.005 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.093 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 | -0.000 | -0.008 |  0.006 |  0.002 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.082 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  0.997 |  1.005 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.065 |  0.070 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.090 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.016 |  0.012 |  0.003 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.089 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.003 |  0.004 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  0.996 |  1.006 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.007 |  0.006 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.086 |  0.079 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 | -0.000 | -0.007 |  0.006 |  0.002 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.078 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.996 |  1.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.004 |  0.004 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.065 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.086 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.009 |  0.013 |  0.002 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.078 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  0.997 |  1.008 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 | -0.006 |  0.004 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.097 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.082 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  0.997 |  1.004 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.006 |  0.005 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.071 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.084 |  0.103 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.006 |  0.004 |  0.001 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.078 |  0.075 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  0.996 |  1.006 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 | -0.004 |  0.007 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.087 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 | -0.005 |  0.007 |  0.002 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.081 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  0.995 |  1.006 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.008 |  0.005 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.061 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.090 |  0.093 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight
 |  0.000 | -0.009 |  0.008 |  0.002 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.083 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.004 |  0.005 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  0.997 |  1.006 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 | -0.005 |  0.006 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.084 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight
 | -0.000 | -0.004 |  0.008 |  0.002 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.088 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  0.996 |  1.006 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight
 | -0.000 | -0.006 |  0.004 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.062 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.078 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.008 |  0.009 |  0.002 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.082 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.001 |  0.997 |  1.009 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.085 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 | -0.005 |  0.008 |  0.002 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.080 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.006 |  0.005 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.030 |  0.031 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight
 | -0.001 | -0.026 |  0.026 |  0.014 | torch.Size([180]) || layers.3.conv.bias
 |  1.000 |  0.997 |  1.007 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight
 |  0.000 | -0.004 |  0.007 |  0.002 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.070 |  0.067 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.090 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight
 | -0.000 | -0.006 |  0.006 |  0.002 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.082 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight
 | -0.000 | -0.004 |  0.004 |  0.002 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  0.996 |  1.006 |  0.002 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.004 |  0.005 |  0.002 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.083 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 | -0.003 |  0.005 |  0.001 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.077 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 | -0.004 |  0.004 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask
 |  1.000 |  0.997 |  1.005 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.004 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.063 |  0.056 |  0.019 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.099 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.003 |  0.006 |  0.001 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.092 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.003 |  0.004 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  0.997 |  1.004 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.003 |  0.005 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.080 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 | -0.004 |  0.005 |  0.001 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.089 |  0.098 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.003 |  0.004 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.997 |  1.004 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.070 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.086 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight
 | -0.000 | -0.004 |  0.003 |  0.001 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.084 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.003 |  0.004 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  0.996 |  1.004 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.004 |  0.004 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.077 |  0.076 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 | -0.005 |  0.004 |  0.001 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.096 |  0.107 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.003 |  0.004 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.064 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.086 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.006 |  0.007 |  0.001 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.081 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight
 |  0.000 | -0.003 |  0.004 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  0.997 |  1.008 |  0.002 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight
 |  0.000 | -0.004 |  0.005 |  0.002 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.085 |  0.091 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.080 |  0.099 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 | -0.004 |  0.005 |  0.002 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  0.997 |  1.005 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.004 |  0.005 |  0.002 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.083 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.087 |  0.095 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.008 |  0.010 |  0.002 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.077 |  0.088 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.004 |  0.004 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  0.996 |  1.004 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.003 |  0.004 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.088 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight
 | -0.000 | -0.003 |  0.004 |  0.001 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.091 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.003 |  0.004 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask
 |  1.000 |  0.995 |  1.004 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.055 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.084 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.004 |  0.005 |  0.001 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.097 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight
 | -0.000 | -0.004 |  0.005 |  0.002 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  0.996 |  1.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight
 |  0.000 | -0.006 |  0.005 |  0.002 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.079 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.083 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 | -0.003 |  0.004 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.032 |  0.031 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight
 | -0.002 | -0.027 |  0.025 |  0.014 | torch.Size([180]) || layers.4.conv.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight
 |  0.000 | -0.003 |  0.004 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.074 |  0.070 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.084 |  0.103 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.005 |  0.005 |  0.001 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.078 |  0.088 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.004 |  0.004 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  0.998 |  1.004 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight
 | -0.000 | -0.004 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.078 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight
 | -0.000 | -0.004 |  0.005 |  0.001 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.092 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.004 |  0.005 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.004 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.076 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.086 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.004 |  0.004 |  0.001 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.079 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.004 |  0.004 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  0.997 |  1.007 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.086 |  0.089 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight
 | -0.000 | -0.003 |  0.004 |  0.001 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.092 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.004 |  0.004 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.003 |  0.005 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.070 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.086 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.004 |  0.006 |  0.001 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.080 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.004 |  0.004 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  0.997 |  1.006 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.004 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.092 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight
 | -0.000 | -0.005 |  0.004 |  0.001 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.086 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.004 |  0.004 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.063 |  0.082 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.086 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.076 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight
 |  0.000 | -0.004 |  0.004 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  0.997 |  1.007 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight
 |  0.000 | -0.004 |  0.004 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.083 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 | -0.005 |  0.006 |  0.001 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.086 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.004 |  0.004 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  0.998 |  1.004 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.063 |  0.077 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.089 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight
 |  0.000 | -0.005 |  0.004 |  0.001 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.075 |  0.090 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight
 |  0.000 | -0.004 |  0.005 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  0.995 |  1.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight
 |  0.000 | -0.003 |  0.006 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.098 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight
 | -0.000 | -0.006 |  0.005 |  0.001 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.085 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight
 | -0.000 | -0.004 |  0.004 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask
 |  1.000 |  0.997 |  1.005 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight
 | -0.000 | -0.004 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.059 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.089 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.087 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight
 |  0.000 | -0.005 |  0.004 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  0.996 |  1.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight
 |  0.000 | -0.003 |  0.005 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.086 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight
 | -0.000 | -0.004 |  0.004 |  0.001 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.089 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 | -0.004 |  0.004 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.034 |  0.034 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight
 | -0.000 | -0.025 |  0.026 |  0.014 | torch.Size([180]) || layers.5.conv.bias
 |  0.995 |  0.974 |  0.999 |  0.004 | torch.Size([180]) || norm.weight
 | -0.000 | -0.015 |  0.020 |  0.005 | torch.Size([180]) || norm.bias
 |  0.000 | -0.028 |  0.028 |  0.014 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight
 | -0.001 | -0.025 |  0.025 |  0.015 | torch.Size([180]) || conv_after_body.bias
 |  0.000 | -0.031 |  0.032 |  0.014 | torch.Size([3, 180, 3, 3]) || conv_last.weight
 |  0.009 | -0.008 |  0.021 |  0.015 | torch.Size([3]) || conv_last.bias

export CUDA_VISIBLE_DEVICES=0,1,2,3
number of GPUs is: 4
LogHandlers setup!
Random seed: 169
Dataset [DatasetSR - train_dataset] is created.
Dataset [DatasetSR - test_dataset] is created.
Pass this initialization! Initialization was done during network definition!
Pass this initialization! Initialization was done during network definition!
Training model [ModelPlain] is created.
Loading model for G [SwinITMO/ITMO TRAINING/models/600_G.pth] ...
Loading model for E [SwinITMO/ITMO TRAINING/models/600_E.pth] ...
Loading optimizerG [SwinITMO/ITMO TRAINING/models/600_optimizerG.pth] ...
/home/slee67/ENV/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/slee67/ENV/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/home/slee67/ENV/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:429: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
23-04-18 15:14:42.538 : <epoch:  0, iter:     601, lr:2.000e-04> G_loss: 5.639e-02 
23-04-18 15:14:43.686 : <epoch:  0, iter:     602, lr:2.000e-04> G_loss: 7.218e-02 
23-04-18 15:14:44.535 : <epoch:  0, iter:     603, lr:2.000e-04> G_loss: 3.749e-02 
23-04-18 15:14:45.649 : <epoch:  0, iter:     604, lr:2.000e-04> G_loss: 5.801e-02 
23-04-18 15:14:46.896 : <epoch:  0, iter:     605, lr:2.000e-04> G_loss: 4.314e-02 
23-04-18 15:14:48.732 : <epoch:  0, iter:     606, lr:2.000e-04> G_loss: 4.301e-02 
23-04-18 15:14:50.361 : <epoch:  0, iter:     607, lr:2.000e-04> G_loss: 6.846e-02 
23-04-18 15:14:52.192 : <epoch:  0, iter:     608, lr:2.000e-04> G_loss: 9.288e-02 
23-04-18 15:14:54.343 : <epoch:  0, iter:     609, lr:2.000e-04> G_loss: 9.758e-02 
23-04-18 15:14:57.282 : <epoch:  0, iter:     610, lr:2.000e-04> G_loss: 9.018e-02 
23-04-18 15:14:59.731 : <epoch:  0, iter:     611, lr:2.000e-04> G_loss: 5.016e-02 
23-04-18 15:15:02.362 : <epoch:  0, iter:     612, lr:2.000e-04> G_loss: 6.657e-02 
23-04-18 15:15:05.245 : <epoch:  0, iter:     613, lr:2.000e-04> G_loss: 5.726e-02 
23-04-18 15:15:08.623 : <epoch:  0, iter:     614, lr:2.000e-04> G_loss: 4.394e-02 
23-04-18 15:15:12.698 : <epoch:  0, iter:     615, lr:2.000e-04> G_loss: 3.759e-02 
23-04-18 15:15:16.067 : <epoch:  0, iter:     616, lr:2.000e-04> G_loss: 4.864e-02 
23-04-18 15:15:19.646 : <epoch:  0, iter:     617, lr:2.000e-04> G_loss: 5.954e-02 
23-04-18 15:15:23.490 : <epoch:  0, iter:     618, lr:2.000e-04> G_loss: 5.359e-02 
23-04-18 15:15:27.197 : <epoch:  0, iter:     619, lr:2.000e-04> G_loss: 4.096e-02 
23-04-18 15:15:31.862 : <epoch:  0, iter:     620, lr:2.000e-04> G_loss: 4.508e-02 
23-04-18 15:15:35.588 : <epoch:  0, iter:     621, lr:2.000e-04> G_loss: 5.501e-02 
23-04-18 15:15:39.257 : <epoch:  0, iter:     622, lr:2.000e-04> G_loss: 5.080e-02 
23-04-18 15:15:42.869 : <epoch:  0, iter:     623, lr:2.000e-04> G_loss: 5.547e-02 
23-04-18 15:15:47.236 : <epoch:  0, iter:     624, lr:2.000e-04> G_loss: 4.194e-02 
23-04-18 15:15:50.739 : <epoch:  0, iter:     625, lr:2.000e-04> G_loss: 4.044e-02 
23-04-18 15:15:54.633 : <epoch:  0, iter:     626, lr:2.000e-04> G_loss: 6.176e-02 
23-04-18 15:15:58.332 : <epoch:  0, iter:     627, lr:2.000e-04> G_loss: 3.884e-02 
23-04-18 15:16:02.249 : <epoch:  0, iter:     628, lr:2.000e-04> G_loss: 4.962e-02 
23-04-18 15:16:06.764 : <epoch:  0, iter:     629, lr:2.000e-04> G_loss: 1.095e-01 
23-04-18 15:16:11.012 : <epoch:  0, iter:     630, lr:2.000e-04> G_loss: 5.478e-02 
23-04-18 15:16:14.596 : <epoch:  0, iter:     631, lr:2.000e-04> G_loss: 5.243e-02 
23-04-18 15:16:18.073 : <epoch:  0, iter:     632, lr:2.000e-04> G_loss: 6.698e-02 
23-04-18 15:16:21.712 : <epoch:  0, iter:     633, lr:2.000e-04> G_loss: 9.580e-02 
23-04-18 15:16:26.214 : <epoch:  0, iter:     634, lr:2.000e-04> G_loss: 4.186e-02 
23-04-18 15:16:29.707 : <epoch:  0, iter:     635, lr:2.000e-04> G_loss: 8.344e-02 
23-04-18 15:16:33.382 : <epoch:  0, iter:     636, lr:2.000e-04> G_loss: 4.880e-02 
23-04-18 15:16:37.153 : <epoch:  0, iter:     637, lr:2.000e-04> G_loss: 7.789e-02 
23-04-18 15:16:40.951 : <epoch:  0, iter:     638, lr:2.000e-04> G_loss: 4.606e-02 
23-04-18 15:16:45.435 : <epoch:  0, iter:     639, lr:2.000e-04> G_loss: 9.261e-02 
23-04-18 15:16:49.134 : <epoch:  0, iter:     640, lr:2.000e-04> G_loss: 7.055e-02 
23-04-18 15:16:52.963 : <epoch:  0, iter:     641, lr:2.000e-04> G_loss: 3.501e-02 
23-04-18 15:16:56.732 : <epoch:  0, iter:     642, lr:2.000e-04> G_loss: 6.328e-02 
23-04-18 15:17:01.309 : <epoch:  0, iter:     643, lr:2.000e-04> G_loss: 3.728e-02 
23-04-18 15:17:04.984 : <epoch:  0, iter:     644, lr:2.000e-04> G_loss: 4.515e-02 
23-04-18 15:17:08.800 : <epoch:  0, iter:     645, lr:2.000e-04> G_loss: 8.089e-02 
23-04-18 15:17:12.541 : <epoch:  0, iter:     646, lr:2.000e-04> G_loss: 3.679e-02 
23-04-18 15:17:15.984 : <epoch:  0, iter:     647, lr:2.000e-04> G_loss: 3.520e-02 
23-04-18 15:17:20.404 : <epoch:  0, iter:     648, lr:2.000e-04> G_loss: 1.025e-01 
23-04-18 15:17:24.175 : <epoch:  0, iter:     649, lr:2.000e-04> G_loss: 4.152e-02 
23-04-18 15:17:27.991 : <epoch:  0, iter:     650, lr:2.000e-04> G_loss: 5.226e-02 
23-04-18 15:17:31.693 : <epoch:  0, iter:     651, lr:2.000e-04> G_loss: 1.256e-01 
23-04-18 15:17:35.402 : <epoch:  0, iter:     652, lr:2.000e-04> G_loss: 6.645e-02 
23-04-18 15:17:40.108 : <epoch:  0, iter:     653, lr:2.000e-04> G_loss: 7.829e-02 
23-04-18 15:17:43.947 : <epoch:  0, iter:     654, lr:2.000e-04> G_loss: 7.045e-02 
23-04-18 15:17:47.716 : <epoch:  0, iter:     655, lr:2.000e-04> G_loss: 3.315e-02 
23-04-18 15:17:51.161 : <epoch:  0, iter:     656, lr:2.000e-04> G_loss: 5.975e-02 
23-04-18 15:17:55.664 : <epoch:  0, iter:     657, lr:2.000e-04> G_loss: 7.468e-02 
23-04-18 15:17:59.467 : <epoch:  0, iter:     658, lr:2.000e-04> G_loss: 4.384e-02 
23-04-18 15:18:03.175 : <epoch:  0, iter:     659, lr:2.000e-04> G_loss: 8.246e-02 
23-04-18 15:18:06.836 : <epoch:  0, iter:     660, lr:2.000e-04> G_loss: 6.256e-02 
23-04-18 15:18:10.794 : <epoch:  0, iter:     661, lr:2.000e-04> G_loss: 7.755e-02 
23-04-18 15:18:15.487 : <epoch:  0, iter:     662, lr:2.000e-04> G_loss: 3.639e-02 
23-04-18 15:18:19.215 : <epoch:  0, iter:     663, lr:2.000e-04> G_loss: 8.972e-02 
23-04-18 15:18:22.791 : <epoch:  0, iter:     664, lr:2.000e-04> G_loss: 6.416e-02 
23-04-18 15:18:26.750 : <epoch:  0, iter:     665, lr:2.000e-04> G_loss: 5.492e-02 
23-04-18 15:18:32.128 : <epoch:  0, iter:     666, lr:2.000e-04> G_loss: 5.173e-02 
23-04-18 15:18:36.578 : <epoch:  0, iter:     667, lr:2.000e-04> G_loss: 4.850e-02 
23-04-18 15:18:40.456 : <epoch:  0, iter:     668, lr:2.000e-04> G_loss: 6.929e-02 
23-04-18 15:18:44.306 : <epoch:  0, iter:     669, lr:2.000e-04> G_loss: 4.140e-02 
23-04-18 15:18:47.806 : <epoch:  0, iter:     670, lr:2.000e-04> G_loss: 4.316e-02 
23-04-18 15:18:51.844 : <epoch:  0, iter:     671, lr:2.000e-04> G_loss: 4.868e-02 
23-04-18 15:18:56.549 : <epoch:  0, iter:     672, lr:2.000e-04> G_loss: 4.466e-02 
23-04-18 15:19:00.126 : <epoch:  0, iter:     673, lr:2.000e-04> G_loss: 3.581e-02 
23-04-18 15:19:03.775 : <epoch:  0, iter:     674, lr:2.000e-04> G_loss: 3.329e-02 
23-04-18 15:19:07.625 : <epoch:  0, iter:     675, lr:2.000e-04> G_loss: 4.414e-02 
23-04-18 15:19:12.354 : <epoch:  0, iter:     676, lr:2.000e-04> G_loss: 3.173e-02 
23-04-18 15:19:16.200 : <epoch:  0, iter:     677, lr:2.000e-04> G_loss: 4.201e-02 
23-04-18 15:19:19.921 : <epoch:  0, iter:     678, lr:2.000e-04> G_loss: 6.322e-02 
23-04-18 15:19:23.617 : <epoch:  0, iter:     679, lr:2.000e-04> G_loss: 6.993e-02 
23-04-18 15:19:26.987 : <epoch:  0, iter:     680, lr:2.000e-04> G_loss: 6.201e-02 
23-04-18 15:19:31.496 : <epoch:  0, iter:     681, lr:2.000e-04> G_loss: 4.371e-02 
23-04-18 15:19:48.803 : <epoch:  0, iter:     682, lr:2.000e-04> G_loss: 4.344e-02 
23-04-18 15:19:52.614 : <epoch:  0, iter:     683, lr:2.000e-04> G_loss: 8.037e-02 
23-04-18 15:19:56.419 : <epoch:  0, iter:     684, lr:2.000e-04> G_loss: 4.295e-02 
23-04-18 15:20:00.117 : <epoch:  0, iter:     685, lr:2.000e-04> G_loss: 4.110e-02 
23-04-18 15:20:04.524 : <epoch:  0, iter:     686, lr:2.000e-04> G_loss: 5.032e-02 
23-04-18 15:20:08.449 : <epoch:  0, iter:     687, lr:2.000e-04> G_loss: 3.705e-02 
23-04-18 15:20:12.259 : <epoch:  0, iter:     688, lr:2.000e-04> G_loss: 6.158e-02 
23-04-18 15:20:15.826 : <epoch:  0, iter:     689, lr:2.000e-04> G_loss: 6.999e-02 
23-04-18 15:20:20.256 : <epoch:  0, iter:     690, lr:2.000e-04> G_loss: 4.856e-02 
23-04-18 15:20:23.974 : <epoch:  0, iter:     691, lr:2.000e-04> G_loss: 3.302e-02 
23-04-18 15:20:27.596 : <epoch:  0, iter:     692, lr:2.000e-04> G_loss: 5.167e-02 
23-04-18 15:20:31.317 : <epoch:  0, iter:     693, lr:2.000e-04> G_loss: 4.796e-02 
23-04-18 15:20:35.008 : <epoch:  0, iter:     694, lr:2.000e-04> G_loss: 4.068e-02 
23-04-18 15:20:39.847 : <epoch:  0, iter:     695, lr:2.000e-04> G_loss: 2.757e-02 
23-04-18 15:20:43.552 : <epoch:  0, iter:     696, lr:2.000e-04> G_loss: 6.417e-02 
23-04-18 15:20:47.450 : <epoch:  0, iter:     697, lr:2.000e-04> G_loss: 7.548e-02 
23-04-18 15:21:05.235 : <epoch:  0, iter:     698, lr:2.000e-04> G_loss: 6.002e-02 
23-04-18 15:21:09.111 : <epoch:  0, iter:     699, lr:2.000e-04> G_loss: 4.221e-02 
23-04-18 15:21:13.576 : <epoch:  0, iter:     700, lr:2.000e-04> G_loss: 6.887e-02 
23-04-18 15:21:13.576 : Saving the model.
23-04-18 15:21:22.440 : <epoch:  0, iter:     701, lr:2.000e-04> G_loss: 3.276e-02 
23-04-18 15:21:26.366 : <epoch:  0, iter:     702, lr:2.000e-04> G_loss: 6.886e-02 
23-04-18 15:21:30.332 : <epoch:  0, iter:     703, lr:2.000e-04> G_loss: 4.058e-02 
23-04-18 15:21:34.757 : <epoch:  0, iter:     704, lr:2.000e-04> G_loss: 4.554e-02 
23-04-18 15:21:38.437 : <epoch:  0, iter:     705, lr:2.000e-04> G_loss: 7.147e-02 
23-04-18 15:21:42.062 : <epoch:  0, iter:     706, lr:2.000e-04> G_loss: 5.716e-02 
23-04-18 15:21:45.546 : <epoch:  0, iter:     707, lr:2.000e-04> G_loss: 4.724e-02 
23-04-18 15:21:50.176 : <epoch:  0, iter:     708, lr:2.000e-04> G_loss: 7.566e-02 
23-04-18 15:21:54.016 : <epoch:  0, iter:     709, lr:2.000e-04> G_loss: 3.307e-02 
23-04-18 15:21:57.767 : <epoch:  0, iter:     710, lr:2.000e-04> G_loss: 1.030e-01 
23-04-18 15:22:01.501 : <epoch:  0, iter:     711, lr:2.000e-04> G_loss: 5.431e-02 
23-04-18 15:22:05.257 : <epoch:  0, iter:     712, lr:2.000e-04> G_loss: 5.236e-02 
23-04-18 15:22:09.750 : <epoch:  0, iter:     713, lr:2.000e-04> G_loss: 6.140e-02 
23-04-18 15:22:21.354 : <epoch:  0, iter:     714, lr:2.000e-04> G_loss: 1.108e-01 
23-04-18 15:22:24.987 : <epoch:  0, iter:     715, lr:2.000e-04> G_loss: 6.460e-02 
23-04-18 15:22:28.614 : <epoch:  0, iter:     716, lr:2.000e-04> G_loss: 6.918e-02 
23-04-18 15:22:32.331 : <epoch:  0, iter:     717, lr:2.000e-04> G_loss: 2.500e-02 
23-04-18 15:22:36.663 : <epoch:  0, iter:     718, lr:2.000e-04> G_loss: 3.588e-02 
23-04-18 15:22:40.522 : <epoch:  0, iter:     719, lr:2.000e-04> G_loss: 6.082e-02 
23-04-18 15:22:44.331 : <epoch:  0, iter:     720, lr:2.000e-04> G_loss: 5.623e-02 
23-04-18 15:22:48.008 : <epoch:  0, iter:     721, lr:2.000e-04> G_loss: 3.676e-02 
23-04-18 15:22:51.638 : <epoch:  0, iter:     722, lr:2.000e-04> G_loss: 6.294e-02 
23-04-18 15:22:56.419 : <epoch:  0, iter:     723, lr:2.000e-04> G_loss: 5.384e-02 
23-04-18 15:23:00.075 : <epoch:  0, iter:     724, lr:2.000e-04> G_loss: 3.582e-02 
23-04-18 15:23:03.724 : <epoch:  0, iter:     725, lr:2.000e-04> G_loss: 4.517e-02 
23-04-18 15:23:07.432 : <epoch:  0, iter:     726, lr:2.000e-04> G_loss: 7.721e-02 
23-04-18 15:23:11.909 : <epoch:  0, iter:     727, lr:2.000e-04> G_loss: 4.396e-02 
23-04-18 15:23:15.435 : <epoch:  0, iter:     728, lr:2.000e-04> G_loss: 2.777e-02 
23-04-18 15:23:19.068 : <epoch:  0, iter:     729, lr:2.000e-04> G_loss: 4.006e-02 
23-04-18 15:23:32.045 : <epoch:  0, iter:     730, lr:2.000e-04> G_loss: 7.820e-02 
23-04-18 15:23:35.609 : <epoch:  0, iter:     731, lr:2.000e-04> G_loss: 2.756e-02 
23-04-18 15:23:40.306 : <epoch:  0, iter:     732, lr:2.000e-04> G_loss: 8.349e-02 
23-04-18 15:23:43.860 : <epoch:  0, iter:     733, lr:2.000e-04> G_loss: 2.392e-02 
23-04-18 15:23:47.636 : <epoch:  0, iter:     734, lr:2.000e-04> G_loss: 5.437e-02 
23-04-18 15:23:51.282 : <epoch:  0, iter:     735, lr:2.000e-04> G_loss: 3.713e-02 
23-04-18 15:23:55.216 : <epoch:  0, iter:     736, lr:2.000e-04> G_loss: 5.121e-02 
23-04-18 15:23:59.585 : <epoch:  0, iter:     737, lr:2.000e-04> G_loss: 3.513e-02 
23-04-18 15:24:03.272 : <epoch:  0, iter:     738, lr:2.000e-04> G_loss: 4.111e-02 
23-04-18 15:24:07.254 : <epoch:  0, iter:     739, lr:2.000e-04> G_loss: 7.454e-02 
23-04-18 15:24:10.776 : <epoch:  0, iter:     740, lr:2.000e-04> G_loss: 2.871e-02 
23-04-18 15:24:15.596 : <epoch:  0, iter:     741, lr:2.000e-04> G_loss: 7.943e-02 
23-04-18 15:24:19.233 : <epoch:  0, iter:     742, lr:2.000e-04> G_loss: 3.292e-02 
23-04-18 15:24:22.628 : <epoch:  0, iter:     743, lr:2.000e-04> G_loss: 5.595e-02 
23-04-18 15:24:26.403 : <epoch:  0, iter:     744, lr:2.000e-04> G_loss: 7.165e-02 
23-04-18 15:24:30.212 : <epoch:  0, iter:     745, lr:2.000e-04> G_loss: 3.611e-02 
23-04-18 15:24:43.517 : <epoch:  0, iter:     746, lr:2.000e-04> G_loss: 3.974e-02 
23-04-18 15:24:47.198 : <epoch:  0, iter:     747, lr:2.000e-04> G_loss: 3.730e-02 
23-04-18 15:24:50.859 : <epoch:  0, iter:     748, lr:2.000e-04> G_loss: 6.383e-02 
23-04-18 15:24:54.724 : <epoch:  0, iter:     749, lr:2.000e-04> G_loss: 3.936e-02 
23-04-18 15:24:58.352 : <epoch:  0, iter:     750, lr:2.000e-04> G_loss: 3.717e-02 
23-04-18 15:25:02.791 : <epoch:  0, iter:     751, lr:2.000e-04> G_loss: 3.144e-02 
23-04-18 15:25:06.326 : <epoch:  0, iter:     752, lr:2.000e-04> G_loss: 3.058e-02 
23-04-18 15:25:10.177 : <epoch:  0, iter:     753, lr:2.000e-04> G_loss: 5.329e-02 
23-04-18 15:25:13.671 : <epoch:  0, iter:     754, lr:2.000e-04> G_loss: 2.863e-02 
23-04-18 15:25:17.468 : <epoch:  0, iter:     755, lr:2.000e-04> G_loss: 3.975e-02 
23-04-18 15:25:21.418 : <epoch:  0, iter:     756, lr:2.000e-04> G_loss: 2.545e-02 
23-04-18 15:25:25.047 : <epoch:  0, iter:     757, lr:2.000e-04> G_loss: 7.168e-02 
23-04-18 15:25:28.579 : <epoch:  0, iter:     758, lr:2.000e-04> G_loss: 7.136e-02 
23-04-18 15:25:32.068 : <epoch:  0, iter:     759, lr:2.000e-04> G_loss: 7.328e-02 
23-04-18 15:25:36.561 : <epoch:  0, iter:     760, lr:2.000e-04> G_loss: 5.765e-02 
23-04-18 15:25:40.475 : <epoch:  0, iter:     761, lr:2.000e-04> G_loss: 2.736e-02 
23-04-18 15:25:56.591 : <epoch:  0, iter:     762, lr:2.000e-04> G_loss: 5.991e-02 
23-04-18 15:26:00.344 : <epoch:  0, iter:     763, lr:2.000e-04> G_loss: 5.214e-02 
23-04-18 15:26:03.897 : <epoch:  0, iter:     764, lr:2.000e-04> G_loss: 6.581e-02 
23-04-18 15:26:08.982 : <epoch:  0, iter:     765, lr:2.000e-04> G_loss: 1.025e-01 
23-04-18 15:26:12.524 : <epoch:  0, iter:     766, lr:2.000e-04> G_loss: 6.852e-02 
23-04-18 15:26:16.331 : <epoch:  0, iter:     767, lr:2.000e-04> G_loss: 2.844e-02 
23-04-18 15:26:20.004 : <epoch:  0, iter:     768, lr:2.000e-04> G_loss: 4.667e-02 
23-04-18 15:26:23.742 : <epoch:  0, iter:     769, lr:2.000e-04> G_loss: 4.049e-02 
23-04-18 15:26:28.337 : <epoch:  0, iter:     770, lr:2.000e-04> G_loss: 3.956e-02 
23-04-18 15:26:32.196 : <epoch:  0, iter:     771, lr:2.000e-04> G_loss: 1.010e-01 
23-04-18 15:26:35.992 : <epoch:  0, iter:     772, lr:2.000e-04> G_loss: 3.617e-02 
23-04-18 15:26:39.911 : <epoch:  0, iter:     773, lr:2.000e-04> G_loss: 3.824e-02 
23-04-18 15:26:44.375 : <epoch:  0, iter:     774, lr:2.000e-04> G_loss: 5.299e-02 
23-04-18 15:26:48.042 : <epoch:  0, iter:     775, lr:2.000e-04> G_loss: 3.942e-02 
23-04-18 15:26:52.249 : <epoch:  0, iter:     776, lr:2.000e-04> G_loss: 4.885e-02 
23-04-18 15:26:55.945 : <epoch:  0, iter:     777, lr:2.000e-04> G_loss: 5.680e-02 
23-04-18 15:27:09.755 : <epoch:  0, iter:     778, lr:2.000e-04> G_loss: 7.603e-02 
23-04-18 15:27:14.306 : <epoch:  0, iter:     779, lr:2.000e-04> G_loss: 4.889e-02 
23-04-18 15:27:18.123 : <epoch:  0, iter:     780, lr:2.000e-04> G_loss: 3.416e-02 
23-04-18 15:27:25.684 : <epoch:  0, iter:     781, lr:2.000e-04> G_loss: 7.305e-02 
23-04-18 15:27:29.377 : <epoch:  0, iter:     782, lr:2.000e-04> G_loss: 8.457e-02 
23-04-18 15:27:33.280 : <epoch:  0, iter:     783, lr:2.000e-04> G_loss: 3.232e-02 
23-04-18 15:27:37.799 : <epoch:  0, iter:     784, lr:2.000e-04> G_loss: 4.621e-02 
23-04-18 15:27:41.721 : <epoch:  0, iter:     785, lr:2.000e-04> G_loss: 9.044e-02 
23-04-18 15:27:45.869 : <epoch:  0, iter:     786, lr:2.000e-04> G_loss: 9.312e-02 
23-04-18 15:27:49.504 : <epoch:  0, iter:     787, lr:2.000e-04> G_loss: 4.655e-02 
23-04-18 15:27:53.120 : <epoch:  0, iter:     788, lr:2.000e-04> G_loss: 6.740e-02 
23-04-18 15:27:57.558 : <epoch:  0, iter:     789, lr:2.000e-04> G_loss: 7.755e-02 
23-04-18 15:28:00.867 : <epoch:  0, iter:     790, lr:2.000e-04> G_loss: 5.976e-02 
23-04-18 15:28:04.503 : <epoch:  0, iter:     791, lr:2.000e-04> G_loss: 8.130e-02 
23-04-18 15:28:08.320 : <epoch:  0, iter:     792, lr:2.000e-04> G_loss: 3.447e-02 
23-04-18 15:28:12.781 : <epoch:  0, iter:     793, lr:2.000e-04> G_loss: 6.501e-02 
23-04-18 15:28:25.560 : <epoch:  0, iter:     794, lr:2.000e-04> G_loss: 9.427e-02 
23-04-18 15:28:29.186 : <epoch:  0, iter:     795, lr:2.000e-04> G_loss: 4.298e-02 
23-04-18 15:28:32.914 : <epoch:  0, iter:     796, lr:2.000e-04> G_loss: 7.378e-02 
23-04-18 15:28:46.746 : <epoch:  0, iter:     797, lr:2.000e-04> G_loss: 4.794e-02 
23-04-18 15:28:51.300 : <epoch:  0, iter:     798, lr:2.000e-04> G_loss: 4.052e-02 
23-04-18 15:28:55.203 : <epoch:  0, iter:     799, lr:2.000e-04> G_loss: 6.768e-02 
23-04-18 15:28:58.822 : <epoch:  0, iter:     800, lr:2.000e-04> G_loss: 3.345e-02 
23-04-18 15:28:58.822 : Saving the model.
23-04-18 15:29:07.842 : <epoch:  0, iter:     801, lr:2.000e-04> G_loss: 6.102e-02 
23-04-18 15:29:12.630 : <epoch:  0, iter:     802, lr:2.000e-04> G_loss: 7.039e-02 
23-04-18 15:29:16.203 : <epoch:  0, iter:     803, lr:2.000e-04> G_loss: 7.128e-02 
23-04-18 15:29:19.730 : <epoch:  0, iter:     804, lr:2.000e-04> G_loss: 5.027e-02 
23-04-18 15:29:23.547 : <epoch:  0, iter:     805, lr:2.000e-04> G_loss: 5.130e-02 
23-04-18 15:29:27.856 : <epoch:  0, iter:     806, lr:2.000e-04> G_loss: 6.239e-02 
23-04-18 15:29:31.596 : <epoch:  0, iter:     807, lr:2.000e-04> G_loss: 7.430e-02 
23-04-18 15:29:35.310 : <epoch:  0, iter:     808, lr:2.000e-04> G_loss: 5.585e-02 
23-04-18 15:29:38.871 : <epoch:  0, iter:     809, lr:2.000e-04> G_loss: 6.615e-02 
23-04-18 15:29:42.523 : <epoch:  0, iter:     810, lr:2.000e-04> G_loss: 7.559e-02 
23-04-18 15:29:46.797 : <epoch:  0, iter:     811, lr:2.000e-04> G_loss: 6.789e-02 
23-04-18 15:29:50.446 : <epoch:  0, iter:     812, lr:2.000e-04> G_loss: 6.959e-02 
23-04-18 15:29:57.646 : <epoch:  0, iter:     813, lr:2.000e-04> G_loss: 3.735e-02 
23-04-18 15:30:00.928 : <epoch:  0, iter:     814, lr:2.000e-04> G_loss: 5.885e-02 
23-04-18 15:30:04.383 : <epoch:  0, iter:     815, lr:2.000e-04> G_loss: 7.784e-02 
23-04-18 15:30:09.147 : <epoch:  0, iter:     816, lr:2.000e-04> G_loss: 4.202e-02 
23-04-18 15:30:12.733 : <epoch:  0, iter:     817, lr:2.000e-04> G_loss: 3.435e-02 
23-04-18 15:30:16.512 : <epoch:  0, iter:     818, lr:2.000e-04> G_loss: 3.992e-02 
23-04-18 15:30:20.138 : <epoch:  0, iter:     819, lr:2.000e-04> G_loss: 5.691e-02 
23-04-18 15:30:24.285 : <epoch:  0, iter:     820, lr:2.000e-04> G_loss: 4.030e-02 
23-04-18 15:30:28.563 : <epoch:  0, iter:     821, lr:2.000e-04> G_loss: 5.047e-02 
23-04-18 15:30:32.410 : <epoch:  0, iter:     822, lr:2.000e-04> G_loss: 1.056e-01 
23-04-18 15:30:36.059 : <epoch:  0, iter:     823, lr:2.000e-04> G_loss: 4.431e-02 
23-04-18 15:30:40.117 : <epoch:  0, iter:     824, lr:2.000e-04> G_loss: 3.330e-02 
23-04-18 15:30:44.691 : <epoch:  0, iter:     825, lr:2.000e-04> G_loss: 3.729e-02 
23-04-18 15:30:55.357 : <epoch:  0, iter:     826, lr:2.000e-04> G_loss: 5.202e-02 
23-04-18 15:30:58.989 : <epoch:  0, iter:     827, lr:2.000e-04> G_loss: 6.159e-02 
23-04-18 15:31:02.817 : <epoch:  0, iter:     828, lr:2.000e-04> G_loss: 3.368e-02 
23-04-18 15:31:13.420 : <epoch:  0, iter:     829, lr:2.000e-04> G_loss: 5.944e-02 
23-04-18 15:31:18.336 : <epoch:  0, iter:     830, lr:2.000e-04> G_loss: 3.422e-02 
23-04-18 15:31:22.222 : <epoch:  0, iter:     831, lr:2.000e-04> G_loss: 9.873e-02 
23-04-18 15:31:26.042 : <epoch:  0, iter:     832, lr:2.000e-04> G_loss: 5.518e-02 
23-04-18 15:31:29.845 : <epoch:  0, iter:     833, lr:2.000e-04> G_loss: 6.243e-02 
23-04-18 15:31:33.360 : <epoch:  0, iter:     834, lr:2.000e-04> G_loss: 5.285e-02 
23-04-18 15:31:38.295 : <epoch:  0, iter:     835, lr:2.000e-04> G_loss: 5.302e-02 
23-04-18 15:31:42.022 : <epoch:  0, iter:     836, lr:2.000e-04> G_loss: 7.492e-02 
23-04-18 15:31:45.483 : <epoch:  0, iter:     837, lr:2.000e-04> G_loss: 7.397e-02 
23-04-18 15:31:49.224 : <epoch:  0, iter:     838, lr:2.000e-04> G_loss: 3.654e-02 
23-04-18 15:31:54.066 : <epoch:  0, iter:     839, lr:2.000e-04> G_loss: 3.125e-02 
23-04-18 15:31:57.772 : <epoch:  0, iter:     840, lr:2.000e-04> G_loss: 2.667e-02 
23-04-18 15:32:01.574 : <epoch:  0, iter:     841, lr:2.000e-04> G_loss: 3.507e-02 
23-04-18 15:32:04.949 : <epoch:  0, iter:     842, lr:2.000e-04> G_loss: 4.471e-02 
23-04-18 15:32:08.735 : <epoch:  0, iter:     843, lr:2.000e-04> G_loss: 3.972e-02 
23-04-18 15:32:13.142 : <epoch:  0, iter:     844, lr:2.000e-04> G_loss: 3.911e-02 
23-04-18 15:32:36.849 : <epoch:  0, iter:     845, lr:2.000e-04> G_loss: 4.349e-02 
23-04-18 15:32:40.231 : <epoch:  0, iter:     846, lr:2.000e-04> G_loss: 3.896e-02 
23-04-18 15:32:43.900 : <epoch:  0, iter:     847, lr:2.000e-04> G_loss: 2.172e-02 
23-04-18 15:32:47.578 : <epoch:  0, iter:     848, lr:2.000e-04> G_loss: 3.426e-02 
23-04-18 15:32:52.005 : <epoch:  0, iter:     849, lr:2.000e-04> G_loss: 5.978e-02 
23-04-18 15:32:56.046 : <epoch:  0, iter:     850, lr:2.000e-04> G_loss: 3.807e-02 
23-04-18 15:32:59.684 : <epoch:  0, iter:     851, lr:2.000e-04> G_loss: 5.669e-02 
23-04-18 15:33:03.273 : <epoch:  0, iter:     852, lr:2.000e-04> G_loss: 4.420e-02 
23-04-18 15:33:06.893 : <epoch:  0, iter:     853, lr:2.000e-04> G_loss: 7.019e-02 
23-04-18 15:33:11.618 : <epoch:  0, iter:     854, lr:2.000e-04> G_loss: 3.372e-02 
23-04-18 15:33:15.162 : <epoch:  0, iter:     855, lr:2.000e-04> G_loss: 3.522e-02 
23-04-18 15:33:18.867 : <epoch:  0, iter:     856, lr:2.000e-04> G_loss: 4.497e-02 
23-04-18 15:33:22.508 : <epoch:  0, iter:     857, lr:2.000e-04> G_loss: 4.593e-02 
23-04-18 15:33:26.880 : <epoch:  0, iter:     858, lr:2.000e-04> G_loss: 3.881e-02 
23-04-18 15:33:30.547 : <epoch:  0, iter:     859, lr:2.000e-04> G_loss: 3.060e-02 
23-04-18 15:33:34.104 : <epoch:  0, iter:     860, lr:2.000e-04> G_loss: 9.883e-02 
23-04-18 15:33:58.331 : <epoch:  0, iter:     861, lr:2.000e-04> G_loss: 6.827e-02 
23-04-18 15:34:01.825 : <epoch:  0, iter:     862, lr:2.000e-04> G_loss: 3.942e-02 
23-04-18 15:34:06.421 : <epoch:  0, iter:     863, lr:2.000e-04> G_loss: 2.968e-02 
23-04-18 15:34:09.973 : <epoch:  0, iter:     864, lr:2.000e-04> G_loss: 5.199e-02 
23-04-18 15:34:13.641 : <epoch:  0, iter:     865, lr:2.000e-04> G_loss: 7.534e-02 
23-04-18 15:34:17.257 : <epoch:  0, iter:     866, lr:2.000e-04> G_loss: 4.083e-02 
23-04-18 15:34:20.953 : <epoch:  0, iter:     867, lr:2.000e-04> G_loss: 2.997e-02 
23-04-18 15:34:25.525 : <epoch:  0, iter:     868, lr:2.000e-04> G_loss: 6.709e-02 
23-04-18 15:34:29.200 : <epoch:  0, iter:     869, lr:2.000e-04> G_loss: 5.509e-02 
23-04-18 15:34:33.036 : <epoch:  0, iter:     870, lr:2.000e-04> G_loss: 5.155e-02 
23-04-18 15:34:36.818 : <epoch:  0, iter:     871, lr:2.000e-04> G_loss: 5.698e-02 
23-04-18 15:34:41.414 : <epoch:  0, iter:     872, lr:2.000e-04> G_loss: 4.583e-02 
23-04-18 15:34:45.133 : <epoch:  0, iter:     873, lr:2.000e-04> G_loss: 4.469e-02 
23-04-18 15:34:48.775 : <epoch:  0, iter:     874, lr:2.000e-04> G_loss: 3.849e-02 
23-04-18 15:34:52.436 : <epoch:  0, iter:     875, lr:2.000e-04> G_loss: 9.853e-02 
23-04-18 15:34:56.146 : <epoch:  0, iter:     876, lr:2.000e-04> G_loss: 4.318e-02 
23-04-18 15:35:27.915 : <epoch:  0, iter:     877, lr:2.000e-04> G_loss: 3.648e-02 
23-04-18 15:35:31.386 : <epoch:  0, iter:     878, lr:2.000e-04> G_loss: 6.055e-02 
23-04-18 15:35:35.048 : <epoch:  0, iter:     879, lr:2.000e-04> G_loss: 3.448e-02 
23-04-18 15:35:38.834 : <epoch:  0, iter:     880, lr:2.000e-04> G_loss: 4.450e-02 
23-04-18 15:35:42.397 : <epoch:  0, iter:     881, lr:2.000e-04> G_loss: 7.131e-02 
23-04-18 15:35:47.054 : <epoch:  0, iter:     882, lr:2.000e-04> G_loss: 3.302e-02 
23-04-18 15:35:50.694 : <epoch:  0, iter:     883, lr:2.000e-04> G_loss: 8.083e-02 
23-04-18 15:35:54.429 : <epoch:  0, iter:     884, lr:2.000e-04> G_loss: 4.384e-02 
23-04-18 15:35:57.929 : <epoch:  0, iter:     885, lr:2.000e-04> G_loss: 4.543e-02 
23-04-18 15:36:01.583 : <epoch:  0, iter:     886, lr:2.000e-04> G_loss: 7.175e-02 
23-04-18 15:36:06.029 : <epoch:  0, iter:     887, lr:2.000e-04> G_loss: 3.046e-02 
23-04-18 15:36:09.666 : <epoch:  0, iter:     888, lr:2.000e-04> G_loss: 4.372e-02 
23-04-18 15:36:13.178 : <epoch:  0, iter:     889, lr:2.000e-04> G_loss: 3.929e-02 
23-04-18 15:36:17.019 : <epoch:  0, iter:     890, lr:2.000e-04> G_loss: 6.003e-02 
23-04-18 15:36:21.194 : <epoch:  0, iter:     891, lr:2.000e-04> G_loss: 4.673e-02 
23-04-18 15:36:24.962 : <epoch:  0, iter:     892, lr:2.000e-04> G_loss: 4.329e-02 
23-04-18 15:36:37.661 : <epoch:  0, iter:     893, lr:2.000e-04> G_loss: 6.752e-02 
23-04-18 15:36:41.528 : <epoch:  0, iter:     894, lr:2.000e-04> G_loss: 3.228e-02 
23-04-18 15:36:44.941 : <epoch:  0, iter:     895, lr:2.000e-04> G_loss: 1.259e-01 
23-04-18 15:36:49.546 : <epoch:  0, iter:     896, lr:2.000e-04> G_loss: 4.211e-02 
23-04-18 15:36:53.241 : <epoch:  0, iter:     897, lr:2.000e-04> G_loss: 4.470e-02 
23-04-18 15:36:56.922 : <epoch:  0, iter:     898, lr:2.000e-04> G_loss: 3.097e-02 
23-04-18 15:37:00.543 : <epoch:  0, iter:     899, lr:2.000e-04> G_loss: 6.650e-02 
23-04-18 15:37:04.077 : <epoch:  0, iter:     900, lr:2.000e-04> G_loss: 3.111e-02 
23-04-18 15:37:04.078 : Saving the model.
23-04-18 15:37:14.635 : <epoch:  0, iter:     901, lr:2.000e-04> G_loss: 3.498e-02 
23-04-18 15:37:18.307 : <epoch:  0, iter:     902, lr:2.000e-04> G_loss: 2.939e-02 
23-04-18 15:37:22.149 : <epoch:  0, iter:     903, lr:2.000e-04> G_loss: 3.568e-02 
23-04-18 15:37:25.812 : <epoch:  0, iter:     904, lr:2.000e-04> G_loss: 4.939e-02 
23-04-18 15:37:30.171 : <epoch:  0, iter:     905, lr:2.000e-04> G_loss: 3.305e-02 
23-04-18 15:37:33.613 : <epoch:  0, iter:     906, lr:2.000e-04> G_loss: 5.944e-02 
23-04-18 15:37:37.086 : <epoch:  0, iter:     907, lr:2.000e-04> G_loss: 4.484e-02 
23-04-18 15:37:40.843 : <epoch:  0, iter:     908, lr:2.000e-04> G_loss: 3.803e-02 
23-04-18 15:37:48.732 : <epoch:  0, iter:     909, lr:2.000e-04> G_loss: 3.974e-02 
23-04-18 15:37:52.456 : <epoch:  0, iter:     910, lr:2.000e-04> G_loss: 6.223e-02 
23-04-18 15:37:56.151 : <epoch:  0, iter:     911, lr:2.000e-04> G_loss: 3.216e-02 
23-04-18 15:37:59.842 : <epoch:  0, iter:     912, lr:2.000e-04> G_loss: 4.047e-02 
23-04-18 15:38:03.552 : <epoch:  0, iter:     913, lr:2.000e-04> G_loss: 4.153e-02 
23-04-18 15:38:08.278 : <epoch:  0, iter:     914, lr:2.000e-04> G_loss: 2.542e-02 
23-04-18 15:38:11.611 : <epoch:  0, iter:     915, lr:2.000e-04> G_loss: 2.755e-02 
23-04-18 15:38:15.389 : <epoch:  0, iter:     916, lr:2.000e-04> G_loss: 3.936e-02 
23-04-18 15:38:19.017 : <epoch:  0, iter:     917, lr:2.000e-04> G_loss: 1.001e-01 
23-04-18 15:38:22.762 : <epoch:  0, iter:     918, lr:2.000e-04> G_loss: 3.918e-02 
23-04-18 15:38:27.346 : <epoch:  0, iter:     919, lr:2.000e-04> G_loss: 8.961e-02 
23-04-18 15:38:31.263 : <epoch:  0, iter:     920, lr:2.000e-04> G_loss: 6.678e-02 
23-04-18 15:38:34.792 : <epoch:  0, iter:     921, lr:2.000e-04> G_loss: 6.266e-02 
23-04-18 15:38:38.639 : <epoch:  0, iter:     922, lr:2.000e-04> G_loss: 4.848e-02 
23-04-18 15:38:42.639 : <epoch:  0, iter:     923, lr:2.000e-04> G_loss: 1.100e-01 
23-04-18 15:38:46.168 : <epoch:  0, iter:     924, lr:2.000e-04> G_loss: 3.432e-02 
23-04-18 15:39:11.708 : <epoch:  0, iter:     925, lr:2.000e-04> G_loss: 5.996e-02 
23-04-18 15:39:15.479 : <epoch:  0, iter:     926, lr:2.000e-04> G_loss: 4.560e-02 
23-04-18 15:39:18.978 : <epoch:  0, iter:     927, lr:2.000e-04> G_loss: 4.627e-02 
23-04-18 15:39:23.264 : <epoch:  0, iter:     928, lr:2.000e-04> G_loss: 4.400e-02 
23-04-18 15:39:26.861 : <epoch:  0, iter:     929, lr:2.000e-04> G_loss: 7.949e-02 
23-04-18 15:39:30.552 : <epoch:  0, iter:     930, lr:2.000e-04> G_loss: 3.730e-02 
23-04-18 15:39:34.296 : <epoch:  0, iter:     931, lr:2.000e-04> G_loss: 3.103e-02 
23-04-18 15:39:38.065 : <epoch:  0, iter:     932, lr:2.000e-04> G_loss: 4.180e-02 
23-04-18 15:39:42.543 : <epoch:  0, iter:     933, lr:2.000e-04> G_loss: 4.284e-02 
23-04-18 15:39:46.183 : <epoch:  0, iter:     934, lr:2.000e-04> G_loss: 3.079e-02 
23-04-18 15:39:49.950 : <epoch:  0, iter:     935, lr:2.000e-04> G_loss: 2.536e-02 
23-04-18 15:39:53.696 : <epoch:  0, iter:     936, lr:2.000e-04> G_loss: 4.060e-02 
23-04-18 15:39:57.393 : <epoch:  0, iter:     937, lr:2.000e-04> G_loss: 5.783e-02 
23-04-18 15:40:01.927 : <epoch:  0, iter:     938, lr:2.000e-04> G_loss: 7.740e-02 
23-04-18 15:40:05.555 : <epoch:  0, iter:     939, lr:2.000e-04> G_loss: 3.165e-02 
23-04-18 15:40:09.201 : <epoch:  0, iter:     940, lr:2.000e-04> G_loss: 3.920e-02 
23-04-18 15:40:30.561 : <epoch:  0, iter:     941, lr:2.000e-04> G_loss: 6.736e-02 
23-04-18 15:40:35.035 : <epoch:  0, iter:     942, lr:2.000e-04> G_loss: 3.137e-02 
23-04-18 15:40:38.550 : <epoch:  0, iter:     943, lr:2.000e-04> G_loss: 2.912e-02 
23-04-18 15:40:42.057 : <epoch:  0, iter:     944, lr:2.000e-04> G_loss: 3.664e-02 
23-04-18 15:40:45.765 : <epoch:  0, iter:     945, lr:2.000e-04> G_loss: 8.783e-02 
23-04-18 15:40:49.260 : <epoch:  0, iter:     946, lr:2.000e-04> G_loss: 4.397e-02 
23-04-18 15:40:53.889 : <epoch:  0, iter:     947, lr:2.000e-04> G_loss: 4.706e-02 
23-04-18 15:40:57.551 : <epoch:  0, iter:     948, lr:2.000e-04> G_loss: 3.239e-02 
23-04-18 15:41:01.271 : <epoch:  0, iter:     949, lr:2.000e-04> G_loss: 3.028e-02 
23-04-18 15:41:04.930 : <epoch:  0, iter:     950, lr:2.000e-04> G_loss: 2.764e-02 
23-04-18 15:41:08.651 : <epoch:  0, iter:     951, lr:2.000e-04> G_loss: 3.870e-02 
23-04-18 15:41:12.814 : <epoch:  0, iter:     952, lr:2.000e-04> G_loss: 4.896e-02 
23-04-18 15:41:16.353 : <epoch:  0, iter:     953, lr:2.000e-04> G_loss: 7.525e-02 
23-04-18 15:41:20.029 : <epoch:  0, iter:     954, lr:2.000e-04> G_loss: 4.927e-02 
23-04-18 15:41:23.942 : <epoch:  0, iter:     955, lr:2.000e-04> G_loss: 3.229e-02 
23-04-18 15:41:28.247 : <epoch:  0, iter:     956, lr:2.000e-04> G_loss: 4.533e-02 
23-04-18 15:41:31.842 : <epoch:  0, iter:     957, lr:2.000e-04> G_loss: 6.948e-02 
23-04-18 15:41:35.425 : <epoch:  0, iter:     958, lr:2.000e-04> G_loss: 6.750e-02 
23-04-18 15:41:39.117 : <epoch:  0, iter:     959, lr:2.000e-04> G_loss: 4.453e-02 
23-04-18 15:41:42.956 : <epoch:  0, iter:     960, lr:2.000e-04> G_loss: 5.484e-02 
23-04-18 15:41:47.352 : <epoch:  0, iter:     961, lr:2.000e-04> G_loss: 3.416e-02 
23-04-18 15:41:51.013 : <epoch:  0, iter:     962, lr:2.000e-04> G_loss: 5.674e-02 
23-04-18 15:41:54.611 : <epoch:  0, iter:     963, lr:2.000e-04> G_loss: 6.002e-02 
23-04-18 15:41:58.307 : <epoch:  0, iter:     964, lr:2.000e-04> G_loss: 6.883e-02 
23-04-18 15:42:01.942 : <epoch:  0, iter:     965, lr:2.000e-04> G_loss: 4.973e-02 
23-04-18 15:42:06.190 : <epoch:  0, iter:     966, lr:2.000e-04> G_loss: 4.181e-02 
23-04-18 15:42:10.155 : <epoch:  0, iter:     967, lr:2.000e-04> G_loss: 6.568e-02 
23-04-18 15:42:13.899 : <epoch:  0, iter:     968, lr:2.000e-04> G_loss: 6.641e-02 
23-04-18 15:42:17.331 : <epoch:  0, iter:     969, lr:2.000e-04> G_loss: 7.144e-02 
23-04-18 15:42:21.109 : <epoch:  0, iter:     970, lr:2.000e-04> G_loss: 8.877e-02 
23-04-18 15:42:25.409 : <epoch:  0, iter:     971, lr:2.000e-04> G_loss: 5.473e-02 
23-04-18 15:42:29.094 : <epoch:  0, iter:     972, lr:2.000e-04> G_loss: 3.239e-02 
23-04-18 15:42:46.375 : <epoch:  0, iter:     973, lr:2.000e-04> G_loss: 4.715e-02 
23-04-18 15:42:49.983 : <epoch:  0, iter:     974, lr:2.000e-04> G_loss: 6.455e-02 
23-04-18 15:42:54.787 : <epoch:  0, iter:     975, lr:2.000e-04> G_loss: 6.433e-02 
23-04-18 15:42:58.422 : <epoch:  0, iter:     976, lr:2.000e-04> G_loss: 4.295e-02 
23-04-18 15:43:02.109 : <epoch:  0, iter:     977, lr:2.000e-04> G_loss: 2.649e-02 
23-04-18 15:43:06.049 : <epoch:  0, iter:     978, lr:2.000e-04> G_loss: 3.179e-02 
23-04-18 15:43:09.857 : <epoch:  0, iter:     979, lr:2.000e-04> G_loss: 5.547e-02 
23-04-18 15:43:14.175 : <epoch:  0, iter:     980, lr:2.000e-04> G_loss: 4.327e-02 
23-04-18 15:43:17.914 : <epoch:  0, iter:     981, lr:2.000e-04> G_loss: 1.111e-01 
23-04-18 15:43:21.570 : <epoch:  0, iter:     982, lr:2.000e-04> G_loss: 5.792e-02 
23-04-18 15:43:25.215 : <epoch:  0, iter:     983, lr:2.000e-04> G_loss: 4.532e-02 
23-04-18 15:43:28.918 : <epoch:  0, iter:     984, lr:2.000e-04> G_loss: 4.108e-02 
23-04-18 15:43:33.062 : <epoch:  0, iter:     985, lr:2.000e-04> G_loss: 6.522e-02 
23-04-18 15:43:36.479 : <epoch:  0, iter:     986, lr:2.000e-04> G_loss: 4.429e-02 
23-04-18 15:43:40.146 : <epoch:  0, iter:     987, lr:2.000e-04> G_loss: 4.077e-02 
23-04-18 15:43:43.694 : <epoch:  0, iter:     988, lr:2.000e-04> G_loss: 3.937e-02 
23-04-18 15:44:07.211 : <epoch:  0, iter:     989, lr:2.000e-04> G_loss: 3.682e-02 
23-04-18 15:44:10.780 : <epoch:  0, iter:     990, lr:2.000e-04> G_loss: 4.552e-02 
23-04-18 15:44:14.371 : <epoch:  0, iter:     991, lr:2.000e-04> G_loss: 3.643e-02 
23-04-18 15:44:17.893 : <epoch:  0, iter:     992, lr:2.000e-04> G_loss: 3.973e-02 
23-04-18 15:44:21.618 : <epoch:  0, iter:     993, lr:2.000e-04> G_loss: 5.634e-02 
23-04-18 15:44:26.009 : <epoch:  0, iter:     994, lr:2.000e-04> G_loss: 2.967e-02 
23-04-18 15:44:29.608 : <epoch:  0, iter:     995, lr:2.000e-04> G_loss: 6.030e-02 
23-04-18 15:44:33.240 : <epoch:  0, iter:     996, lr:2.000e-04> G_loss: 2.996e-02 
23-04-18 15:44:36.988 : <epoch:  0, iter:     997, lr:2.000e-04> G_loss: 5.782e-02 
23-04-18 15:44:40.851 : <epoch:  0, iter:     998, lr:2.000e-04> G_loss: 3.829e-02 
23-04-18 15:44:45.196 : <epoch:  0, iter:     999, lr:2.000e-04> G_loss: 6.812e-02 
23-04-18 15:44:48.776 : <epoch:  0, iter:   1,000, lr:2.000e-04> G_loss: 4.057e-02 
23-04-18 15:44:48.776 : Saving the model.
23-04-18 15:44:57.833 : <epoch:  0, iter:   1,001, lr:2.000e-04> G_loss: 4.823e-02 
23-04-18 15:45:01.601 : <epoch:  0, iter:   1,002, lr:2.000e-04> G_loss: 5.687e-02 
23-04-18 15:45:06.487 : <epoch:  0, iter:   1,003, lr:2.000e-04> G_loss: 4.046e-02 
23-04-18 15:45:10.040 : <epoch:  0, iter:   1,004, lr:2.000e-04> G_loss: 1.249e-01 
23-04-18 15:45:24.950 : <epoch:  0, iter:   1,005, lr:2.000e-04> G_loss: 6.440e-02 
23-04-18 15:45:28.623 : <epoch:  0, iter:   1,006, lr:2.000e-04> G_loss: 7.930e-02 
23-04-18 15:45:33.240 : <epoch:  0, iter:   1,007, lr:2.000e-04> G_loss: 4.231e-02 
23-04-18 15:45:36.821 : <epoch:  0, iter:   1,008, lr:2.000e-04> G_loss: 3.200e-02 
23-04-18 15:45:40.582 : <epoch:  0, iter:   1,009, lr:2.000e-04> G_loss: 4.380e-02 
23-04-18 15:45:44.397 : <epoch:  0, iter:   1,010, lr:2.000e-04> G_loss: 4.437e-02 
23-04-18 15:45:48.020 : <epoch:  0, iter:   1,011, lr:2.000e-04> G_loss: 8.076e-02 
23-04-18 15:45:52.611 : <epoch:  0, iter:   1,012, lr:2.000e-04> G_loss: 4.299e-02 
23-04-18 15:45:56.425 : <epoch:  0, iter:   1,013, lr:2.000e-04> G_loss: 6.100e-02 
23-04-18 15:46:00.014 : <epoch:  0, iter:   1,014, lr:2.000e-04> G_loss: 6.244e-02 
23-04-18 15:46:03.665 : <epoch:  0, iter:   1,015, lr:2.000e-04> G_loss: 6.355e-02 
23-04-18 15:46:07.443 : <epoch:  0, iter:   1,016, lr:2.000e-04> G_loss: 4.027e-02 
23-04-18 15:46:11.925 : <epoch:  0, iter:   1,017, lr:2.000e-04> G_loss: 5.742e-02 
23-04-18 15:46:15.591 : <epoch:  0, iter:   1,018, lr:2.000e-04> G_loss: 5.013e-02 
23-04-18 15:46:19.218 : <epoch:  0, iter:   1,019, lr:2.000e-04> G_loss: 4.476e-02 
23-04-18 15:46:22.678 : <epoch:  0, iter:   1,020, lr:2.000e-04> G_loss: 7.861e-02 
23-04-18 15:46:33.469 : <epoch:  0, iter:   1,021, lr:2.000e-04> G_loss: 5.953e-02 
23-04-18 15:46:36.966 : <epoch:  0, iter:   1,022, lr:2.000e-04> G_loss: 4.993e-02 
23-04-18 15:46:40.843 : <epoch:  0, iter:   1,023, lr:2.000e-04> G_loss: 5.664e-02 
23-04-18 15:46:44.593 : <epoch:  0, iter:   1,024, lr:2.000e-04> G_loss: 3.343e-02 
23-04-18 15:46:48.353 : <epoch:  0, iter:   1,025, lr:2.000e-04> G_loss: 5.197e-02 
23-04-18 15:46:52.742 : <epoch:  0, iter:   1,026, lr:2.000e-04> G_loss: 5.428e-02 
23-04-18 15:46:56.487 : <epoch:  0, iter:   1,027, lr:2.000e-04> G_loss: 5.476e-02 
23-04-18 15:47:00.232 : <epoch:  0, iter:   1,028, lr:2.000e-04> G_loss: 4.526e-02 
23-04-18 15:47:03.849 : <epoch:  0, iter:   1,029, lr:2.000e-04> G_loss: 5.516e-02 
23-04-18 15:47:07.505 : <epoch:  0, iter:   1,030, lr:2.000e-04> G_loss: 3.628e-02 
23-04-18 15:47:12.245 : <epoch:  0, iter:   1,031, lr:2.000e-04> G_loss: 4.120e-02 
23-04-18 15:47:15.857 : <epoch:  0, iter:   1,032, lr:2.000e-04> G_loss: 4.578e-02 
23-04-18 15:47:19.498 : <epoch:  0, iter:   1,033, lr:2.000e-04> G_loss: 3.782e-02 
23-04-18 15:47:23.036 : <epoch:  0, iter:   1,034, lr:2.000e-04> G_loss: 4.330e-02 
23-04-18 15:47:26.637 : <epoch:  0, iter:   1,035, lr:2.000e-04> G_loss: 4.923e-02 
23-04-18 15:47:31.193 : <epoch:  0, iter:   1,036, lr:2.000e-04> G_loss: 5.290e-02 
23-04-18 15:47:43.158 : <epoch:  0, iter:   1,037, lr:2.000e-04> G_loss: 3.058e-02 
23-04-18 15:47:46.882 : <epoch:  0, iter:   1,038, lr:2.000e-04> G_loss: 4.806e-02 
23-04-18 15:47:50.517 : <epoch:  0, iter:   1,039, lr:2.000e-04> G_loss: 6.298e-02 
23-04-18 15:47:55.247 : <epoch:  0, iter:   1,040, lr:2.000e-04> G_loss: 5.974e-02 
23-04-18 15:47:58.905 : <epoch:  0, iter:   1,041, lr:2.000e-04> G_loss: 1.003e-01 
23-04-18 15:48:02.365 : <epoch:  0, iter:   1,042, lr:2.000e-04> G_loss: 4.090e-02 
23-04-18 15:48:06.200 : <epoch:  0, iter:   1,043, lr:2.000e-04> G_loss: 7.085e-02 
23-04-18 15:48:09.972 : <epoch:  0, iter:   1,044, lr:2.000e-04> G_loss: 4.737e-02 
23-04-18 15:48:14.366 : <epoch:  0, iter:   1,045, lr:2.000e-04> G_loss: 7.106e-02 
23-04-18 15:48:17.973 : <epoch:  0, iter:   1,046, lr:2.000e-04> G_loss: 5.953e-02 
23-04-18 15:48:21.677 : <epoch:  0, iter:   1,047, lr:2.000e-04> G_loss: 4.347e-02 
23-04-18 15:48:25.353 : <epoch:  0, iter:   1,048, lr:2.000e-04> G_loss: 4.507e-02 
23-04-18 15:48:29.027 : <epoch:  0, iter:   1,049, lr:2.000e-04> G_loss: 5.279e-02 
23-04-18 15:48:43.025 : <epoch:  0, iter:   1,050, lr:2.000e-04> G_loss: 4.576e-02 
23-04-18 15:48:46.500 : <epoch:  0, iter:   1,051, lr:2.000e-04> G_loss: 3.776e-02 
23-04-18 15:48:50.057 : <epoch:  0, iter:   1,052, lr:2.000e-04> G_loss: 4.870e-02 
23-04-18 15:48:53.607 : <epoch:  0, iter:   1,053, lr:2.000e-04> G_loss: 4.779e-02 
23-04-18 15:48:58.000 : <epoch:  0, iter:   1,054, lr:2.000e-04> G_loss: 5.266e-02 
23-04-18 15:49:01.521 : <epoch:  0, iter:   1,055, lr:2.000e-04> G_loss: 6.121e-02 
23-04-18 15:49:05.357 : <epoch:  0, iter:   1,056, lr:2.000e-04> G_loss: 3.776e-02 
23-04-18 15:49:09.158 : <epoch:  0, iter:   1,057, lr:2.000e-04> G_loss: 3.824e-02 
23-04-18 15:49:12.670 : <epoch:  0, iter:   1,058, lr:2.000e-04> G_loss: 1.004e-01 
23-04-18 15:49:17.105 : <epoch:  0, iter:   1,059, lr:2.000e-04> G_loss: 6.696e-02 
23-04-18 15:49:20.820 : <epoch:  0, iter:   1,060, lr:2.000e-04> G_loss: 4.818e-02 
23-04-18 15:49:24.742 : <epoch:  0, iter:   1,061, lr:2.000e-04> G_loss: 6.053e-02 
23-04-18 15:49:28.343 : <epoch:  0, iter:   1,062, lr:2.000e-04> G_loss: 5.603e-02 
23-04-18 15:49:32.038 : <epoch:  0, iter:   1,063, lr:2.000e-04> G_loss: 4.266e-02 
23-04-18 15:49:36.540 : <epoch:  0, iter:   1,064, lr:2.000e-04> G_loss: 4.938e-02 
23-04-18 15:49:40.427 : <epoch:  0, iter:   1,065, lr:2.000e-04> G_loss: 3.328e-02 
23-04-18 15:49:57.005 : <epoch:  0, iter:   1,066, lr:2.000e-04> G_loss: 4.529e-02 
23-04-18 15:50:00.684 : <epoch:  0, iter:   1,067, lr:2.000e-04> G_loss: 3.627e-02 
23-04-18 15:50:03.828 : <epoch:  0, iter:   1,068, lr:2.000e-04> G_loss: 6.043e-02 
23-04-18 15:50:08.480 : <epoch:  0, iter:   1,069, lr:2.000e-04> G_loss: 6.039e-02 
23-04-18 15:50:12.147 : <epoch:  0, iter:   1,070, lr:2.000e-04> G_loss: 3.494e-02 
23-04-18 15:50:15.529 : <epoch:  0, iter:   1,071, lr:2.000e-04> G_loss: 2.842e-02 
23-04-18 15:50:19.210 : <epoch:  0, iter:   1,072, lr:2.000e-04> G_loss: 7.436e-02 
23-04-18 15:50:23.683 : <epoch:  0, iter:   1,073, lr:2.000e-04> G_loss: 1.109e-01 
23-04-18 15:50:27.341 : <epoch:  0, iter:   1,074, lr:2.000e-04> G_loss: 5.802e-02 
23-04-18 15:50:31.070 : <epoch:  0, iter:   1,075, lr:2.000e-04> G_loss: 7.410e-02 
23-04-18 15:50:34.768 : <epoch:  0, iter:   1,076, lr:2.000e-04> G_loss: 6.238e-02 
23-04-18 15:50:38.435 : <epoch:  0, iter:   1,077, lr:2.000e-04> G_loss: 4.509e-02 
23-04-18 15:50:43.093 : <epoch:  0, iter:   1,078, lr:2.000e-04> G_loss: 2.693e-02 
23-04-18 15:50:46.697 : <epoch:  0, iter:   1,079, lr:2.000e-04> G_loss: 3.819e-02 
23-04-18 15:50:50.351 : <epoch:  0, iter:   1,080, lr:2.000e-04> G_loss: 3.248e-02 
23-04-18 15:50:54.064 : <epoch:  0, iter:   1,081, lr:2.000e-04> G_loss: 5.881e-02 
23-04-18 15:50:57.862 : <epoch:  0, iter:   1,082, lr:2.000e-04> G_loss: 8.668e-02 
23-04-18 15:51:02.235 : <epoch:  0, iter:   1,083, lr:2.000e-04> G_loss: 3.723e-02 
23-04-18 15:51:05.762 : <epoch:  0, iter:   1,084, lr:2.000e-04> G_loss: 6.561e-02 
23-04-18 15:51:15.966 : <epoch:  0, iter:   1,085, lr:2.000e-04> G_loss: 6.995e-02 
23-04-18 15:51:19.572 : <epoch:  0, iter:   1,086, lr:2.000e-04> G_loss: 3.661e-02 
23-04-18 15:51:24.105 : <epoch:  0, iter:   1,087, lr:2.000e-04> G_loss: 6.996e-02 
23-04-18 15:51:27.833 : <epoch:  0, iter:   1,088, lr:2.000e-04> G_loss: 3.744e-02 
23-04-18 15:51:31.579 : <epoch:  0, iter:   1,089, lr:2.000e-04> G_loss: 2.905e-02 
23-04-18 15:51:35.077 : <epoch:  0, iter:   1,090, lr:2.000e-04> G_loss: 3.728e-02 
23-04-18 15:51:38.690 : <epoch:  0, iter:   1,091, lr:2.000e-04> G_loss: 3.951e-02 
23-04-18 15:51:43.255 : <epoch:  0, iter:   1,092, lr:2.000e-04> G_loss: 3.595e-02 
23-04-18 15:51:46.945 : <epoch:  0, iter:   1,093, lr:2.000e-04> G_loss: 4.338e-02 
23-04-18 15:51:50.564 : <epoch:  0, iter:   1,094, lr:2.000e-04> G_loss: 3.652e-02 
23-04-18 15:51:54.448 : <epoch:  0, iter:   1,095, lr:2.000e-04> G_loss: 5.271e-02 
23-04-18 15:51:58.980 : <epoch:  0, iter:   1,096, lr:2.000e-04> G_loss: 6.471e-02 
23-04-18 15:52:03.369 : <epoch:  0, iter:   1,097, lr:2.000e-04> G_loss: 2.970e-02 
23-04-18 15:52:11.939 : <epoch:  0, iter:   1,098, lr:2.000e-04> G_loss: 4.079e-02 
23-04-18 15:52:20.688 : <epoch:  0, iter:   1,099, lr:2.000e-04> G_loss: 3.252e-02 
23-04-18 15:52:24.590 : <epoch:  0, iter:   1,100, lr:2.000e-04> G_loss: 4.202e-02 
23-04-18 15:52:24.591 : Saving the model.
23-04-18 15:52:35.682 : <epoch:  0, iter:   1,101, lr:2.000e-04> G_loss: 3.436e-02 
23-04-18 15:52:39.432 : <epoch:  0, iter:   1,102, lr:2.000e-04> G_loss: 5.200e-02 
23-04-18 15:52:42.883 : <epoch:  0, iter:   1,103, lr:2.000e-04> G_loss: 6.057e-02 
23-04-18 15:52:46.452 : <epoch:  0, iter:   1,104, lr:2.000e-04> G_loss: 3.996e-02 
23-04-18 15:52:50.440 : <epoch:  0, iter:   1,105, lr:2.000e-04> G_loss: 6.044e-02 
23-04-18 15:52:54.423 : <epoch:  0, iter:   1,106, lr:2.000e-04> G_loss: 3.216e-02 
23-04-18 15:52:58.056 : <epoch:  0, iter:   1,107, lr:2.000e-04> G_loss: 2.644e-02 
23-04-18 15:53:01.378 : <epoch:  0, iter:   1,108, lr:2.000e-04> G_loss: 7.189e-02 
23-04-18 15:53:09.722 : <epoch:  0, iter:   1,109, lr:2.000e-04> G_loss: 2.985e-02 
23-04-18 15:53:14.166 : <epoch:  0, iter:   1,110, lr:2.000e-04> G_loss: 3.858e-02 
23-04-18 15:53:17.839 : <epoch:  0, iter:   1,111, lr:2.000e-04> G_loss: 2.878e-02 
23-04-18 15:53:21.571 : <epoch:  0, iter:   1,112, lr:2.000e-04> G_loss: 4.297e-02 
23-04-18 15:53:25.150 : <epoch:  0, iter:   1,113, lr:2.000e-04> G_loss: 6.151e-02 
23-04-18 15:53:28.735 : <epoch:  0, iter:   1,114, lr:2.000e-04> G_loss: 5.586e-02 
23-04-18 15:53:41.361 : <epoch:  0, iter:   1,115, lr:2.000e-04> G_loss: 2.829e-02 
23-04-18 15:53:44.819 : <epoch:  0, iter:   1,116, lr:2.000e-04> G_loss: 6.193e-02 
23-04-18 15:53:49.727 : <epoch:  0, iter:   1,117, lr:2.000e-04> G_loss: 3.890e-02 
23-04-18 15:53:53.465 : <epoch:  0, iter:   1,118, lr:2.000e-04> G_loss: 6.277e-02 
23-04-18 15:53:57.350 : <epoch:  0, iter:   1,119, lr:2.000e-04> G_loss: 5.018e-02 
23-04-18 15:54:01.628 : <epoch:  0, iter:   1,120, lr:2.000e-04> G_loss: 5.766e-02 
23-04-18 15:54:05.252 : <epoch:  0, iter:   1,121, lr:2.000e-04> G_loss: 4.328e-02 
23-04-18 15:54:08.978 : <epoch:  0, iter:   1,122, lr:2.000e-04> G_loss: 3.343e-02 
23-04-18 15:54:12.605 : <epoch:  0, iter:   1,123, lr:2.000e-04> G_loss: 3.746e-02 
23-04-18 15:54:17.137 : <epoch:  0, iter:   1,124, lr:2.000e-04> G_loss: 3.302e-02 
23-04-18 15:54:20.755 : <epoch:  0, iter:   1,125, lr:2.000e-04> G_loss: 4.968e-02 
23-04-18 15:54:24.521 : <epoch:  0, iter:   1,126, lr:2.000e-04> G_loss: 3.607e-02 
23-04-18 15:54:27.976 : <epoch:  0, iter:   1,127, lr:2.000e-04> G_loss: 5.271e-02 
23-04-18 15:54:33.268 : <epoch:  0, iter:   1,128, lr:2.000e-04> G_loss: 7.546e-02 
23-04-18 15:54:37.672 : <epoch:  0, iter:   1,129, lr:2.000e-04> G_loss: 7.789e-02 
23-04-18 15:54:41.460 : <epoch:  0, iter:   1,130, lr:2.000e-04> G_loss: 5.660e-02 
23-04-18 15:54:49.715 : <epoch:  0, iter:   1,131, lr:2.000e-04> G_loss: 5.842e-02 
23-04-18 15:54:53.752 : <epoch:  0, iter:   1,132, lr:2.000e-04> G_loss: 5.783e-02 
23-04-18 15:55:14.321 : <epoch:  0, iter:   1,133, lr:2.000e-04> G_loss: 6.078e-02 
23-04-18 15:55:18.848 : <epoch:  0, iter:   1,134, lr:2.000e-04> G_loss: 3.870e-02 
23-04-18 15:55:22.365 : <epoch:  0, iter:   1,135, lr:2.000e-04> G_loss: 7.716e-02 
23-04-18 15:55:26.086 : <epoch:  0, iter:   1,136, lr:2.000e-04> G_loss: 3.054e-02 
23-04-18 15:55:29.383 : <epoch:  0, iter:   1,137, lr:2.000e-04> G_loss: 3.374e-02 
23-04-18 15:55:33.907 : <epoch:  0, iter:   1,138, lr:2.000e-04> G_loss: 3.993e-02 
23-04-18 15:55:37.713 : <epoch:  0, iter:   1,139, lr:2.000e-04> G_loss: 3.856e-02 
23-04-18 15:55:41.491 : <epoch:  0, iter:   1,140, lr:2.000e-04> G_loss: 4.975e-02 
23-04-18 15:55:45.182 : <epoch:  0, iter:   1,141, lr:2.000e-04> G_loss: 7.017e-02 
23-04-18 15:55:48.982 : <epoch:  0, iter:   1,142, lr:2.000e-04> G_loss: 4.126e-02 
Traceback (most recent call last):
  File "/home/slee67/KAIR/main_train_psnr.py", line 253, in <module>
    main()
  File "/home/slee67/KAIR/main_train_psnr.py", line 171, in main
    for i, train_data in enumerate(train_loader):
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 628, in __next__
    data = self._next_data()
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1313, in _next_data
    return self._process_data(data)
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1359, in _process_data
    data.reraise()
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/_utils.py", line 543, in reraise
    raise exception
IndexError: Caught IndexError in DataLoader worker process 14.
Original Traceback (most recent call last):
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 302, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 58, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 58, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/slee67/KAIR/data/dataset_itmo.py", line 55, in __getitem__
    H_path = self.paths_H[index]
IndexError: list index out of range

