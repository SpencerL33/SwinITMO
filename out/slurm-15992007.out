
Due to MODULEPATH changes, the following have been reloaded:
  1) libfabric/1.10.1     2) openmpi/4.0.3     3) ucx/1.8.0


Lmod is automatically replacing "intel/2020.1.217" with "gcc/9.3.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) blis/0.8.1     2) cuda/11.4     3) flexiblas/3.0.4     4) openmpi/4.0.3

Tue Apr 18 04:52:32 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  Off  | 00000000:01:00.0 Off |                    0 |
| N/A   31C    P0    52W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM...  Off  | 00000000:41:00.0 Off |                    0 |
| N/A   30C    P0    55W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A100-SXM...  Off  | 00000000:81:00.0 Off |                    0 |
| N/A   31C    P0    54W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A100-SXM...  Off  | 00000000:C1:00.0 Off |                    0 |
| N/A   29C    P0    54W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Testing...
23-04-18 04:52:36.788 :   task: ITMO TRAINING
  model: plain
  gpu_ids: [0, 1, 2, 3]
  dist: False
  scale: 1
  n_channels: 3
  path:[
    root: SwinITMO
    pretrained_netG: SwinITMO/ITMO TRAINING/models/100_G.pth
    pretrained_netE: SwinITMO/ITMO TRAINING/models/100_E.pth
    task: SwinITMO/ITMO TRAINING
    log: SwinITMO/ITMO TRAINING
    options: SwinITMO/ITMO TRAINING/options
    models: SwinITMO/ITMO TRAINING/models
    images: SwinITMO/ITMO TRAINING/images
    pretrained_optimizerG: SwinITMO/ITMO TRAINING/models/100_optimizerG.pth
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: itmo
      dataroot: /home/slee67/KAIR/train_itmo_dirs.txt
      H_size: 64
      dataloader_shuffle: True
      dataloader_num_workers: 16
      dataloader_batch_size: 8
      phase: train
      scale: 1
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: itmo
      dataroot: /home/slee67/KAIR/test_itmo_dirs.txt
      phase: test
      scale: 1
      n_channels: 3
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 1
    in_chans: 3
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: null
    resi_connection: 1conv
    init_type: default
    scale: 1
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 100
    checkpoint_save: 100
    checkpoint_print: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: /home/slee67/KAIR/options/swinir/train_swinir_hdr.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  use_static_graph: False
  num_gpu: 4
  rank: 0
  world_size: 1

23-04-18 04:52:36.906 : Number of train images: 15,449, iters: 1,932
/home/slee67/ENV/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/slee67/ENV/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /home/coulombc/wheels_builder/tmp.17380/python-3.10/torch/aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
23-04-18 04:52:41.286 : 
Networks name: SwinIR
Params number: 11504163
Net structure:
SwinIR(
  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.003)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.006)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.009)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.011)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.014)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.017)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.020)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.023)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.026)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.029)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.031)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.034)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.037)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.040)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.043)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.046)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.049)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.051)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.054)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.057)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.060)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.063)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.066)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.069)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.071)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.074)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.077)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.080)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.083)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.086)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.089)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.091)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.094)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.097)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.100)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_last): Conv2d(180, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

23-04-18 04:52:41.367 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.001 | -0.195 |  0.193 |  0.110 | torch.Size([180, 3, 3, 3]) || conv_first.weight
 |  0.010 | -0.193 |  0.192 |  0.114 | torch.Size([180]) || conv_first.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || patch_embed.norm.weight
 |  0.000 | -0.004 |  0.003 |  0.001 | torch.Size([180]) || patch_embed.norm.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.054 |  0.070 |  0.019 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.096 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.088 |  0.075 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight
 | -0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.092 |  0.091 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.088 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  0.997 |  1.004 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.067 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.088 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.079 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.082 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.085 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.074 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.093 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 | -0.000 | -0.004 |  0.003 |  0.001 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.081 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 | -0.003 |  0.002 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.095 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.091 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.063 |  0.070 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.083 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.004 |  0.003 |  0.001 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.088 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  0.996 |  1.002 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.082 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 | -0.000 | -0.003 |  0.004 |  0.001 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.079 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.059 |  0.072 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.090 |  0.094 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.081 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  0.996 |  1.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.091 |  0.077 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.084 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.061 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.093 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.076 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight
 | -0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.079 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight
 | -0.000 | -0.004 |  0.003 |  0.001 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.083 |  0.089 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.028 |  0.028 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight
 | -0.003 | -0.026 |  0.026 |  0.014 | torch.Size([180]) || layers.0.conv.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.073 |  0.067 |  0.019 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.086 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.075 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.086 |  0.095 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.084 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  0.998 |  1.002 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.073 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.086 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.003 |  0.004 |  0.001 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.080 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.085 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.084 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.058 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.088 |  0.094 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.087 |  0.074 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.090 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.091 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.074 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.096 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.089 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.091 |  0.092 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.079 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight
 | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.067 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.093 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.004 |  0.004 |  0.001 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.082 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.085 |  0.092 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.086 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.004 |  0.004 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.062 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.086 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.083 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.091 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.085 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.028 |  0.028 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight
 |  0.001 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || layers.1.conv.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.062 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.083 |  0.095 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.081 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight
 | -0.000 | -0.002 |  0.002 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.087 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.081 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.066 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.089 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.079 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.085 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.078 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.068 |  0.073 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.092 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.004 |  0.003 |  0.001 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.081 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.082 |  0.099 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.081 |  0.078 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  0.998 |  1.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.064 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.088 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.090 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.074 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.091 |  0.096 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.068 |  0.065 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.082 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.085 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  0.998 |  1.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.091 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.092 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.074 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.093 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.004 |  0.003 |  0.001 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.081 |  0.092 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.002 |  0.002 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.093 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.084 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.028 |  0.028 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight
 |  0.001 | -0.025 |  0.027 |  0.014 | torch.Size([180]) || layers.2.conv.bias
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 | -0.003 |  0.002 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.069 |  0.058 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.086 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.081 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.093 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.080 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  0.998 |  1.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.065 |  0.070 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.091 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.088 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.084 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.079 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.065 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.086 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.078 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.097 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.083 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.071 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.084 |  0.103 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.076 |  0.075 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.086 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.082 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  0.998 |  1.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.004 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.060 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.087 |  0.094 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.083 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.084 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.087 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.062 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.078 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight
 | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.083 |  0.090 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.084 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.082 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.028 |  0.028 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight
 | -0.001 | -0.025 |  0.026 |  0.014 | torch.Size([180]) || layers.3.conv.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.070 |  0.067 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.087 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.082 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight
 | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.003 |  0.002 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.083 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.077 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.003 |  0.002 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.063 |  0.056 |  0.019 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.099 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.003 |  0.002 |  0.001 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.092 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  0.998 |  1.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.079 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.089 |  0.098 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.002 |  0.002 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.070 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.086 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight
 | -0.000 | -0.002 |  0.003 |  0.001 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.083 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  0.998 |  1.002 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.078 |  0.076 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight
 | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.097 |  0.107 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask
 |  1.000 |  0.996 |  1.002 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight
 |  0.000 | -0.003 |  0.002 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.063 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.085 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.081 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.084 |  0.091 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.080 |  0.098 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight
 | -0.000 | -0.002 |  0.002 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.084 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.086 |  0.095 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.002 |  0.003 |  0.001 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.078 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight
 |  0.000 | -0.002 |  0.002 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  0.998 |  1.002 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.089 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.091 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.055 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.084 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.097 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  0.998 |  1.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.079 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.079 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.029 |  0.029 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight
 | -0.002 | -0.026 |  0.025 |  0.014 | torch.Size([180]) || layers.4.conv.bias
 |  1.000 |  0.998 |  1.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.002 |  0.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.074 |  0.070 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.084 |  0.103 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.078 |  0.088 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  0.998 |  1.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.078 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.091 |  0.089 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.002 |  0.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.076 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.086 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.079 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.087 |  0.089 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight
 | -0.000 | -0.002 |  0.002 |  0.001 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.091 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.003 |  0.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.998 |  1.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.069 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.086 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.002 |  0.002 |  0.001 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.080 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  0.997 |  1.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.002 |  0.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.092 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.086 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask
 |  1.000 |  0.998 |  1.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.002 |  0.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.064 |  0.082 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.086 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.002 |  0.002 |  0.001 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.076 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight
 |  0.000 | -0.003 |  0.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.084 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.085 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  0.998 |  1.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.002 |  0.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.063 |  0.077 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.088 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight
 |  0.000 | -0.002 |  0.002 |  0.001 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.075 |  0.090 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.002 |  0.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.099 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight
 | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.085 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight
 | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask
 |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight
 | -0.000 | -0.002 |  0.002 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.060 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.089 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.002 |  0.002 |  0.001 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.087 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  0.998 |  1.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.002 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.086 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight
 | -0.000 | -0.002 |  0.002 |  0.001 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.090 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.030 |  0.030 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight
 | -0.000 | -0.026 |  0.026 |  0.014 | torch.Size([180]) || layers.5.conv.bias
 |  0.997 |  0.993 |  0.999 |  0.001 | torch.Size([180]) || norm.weight
 |  0.000 | -0.004 |  0.006 |  0.002 | torch.Size([180]) || norm.bias
 |  0.000 | -0.027 |  0.027 |  0.014 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight
 | -0.001 | -0.025 |  0.025 |  0.015 | torch.Size([180]) || conv_after_body.bias
 |  0.000 | -0.029 |  0.028 |  0.014 | torch.Size([3, 180, 3, 3]) || conv_last.weight
 |  0.009 | -0.008 |  0.021 |  0.015 | torch.Size([3]) || conv_last.bias

export CUDA_VISIBLE_DEVICES=0,1,2,3
number of GPUs is: 4
LogHandlers setup!
Random seed: 4849
Dataset [DatasetSR - train_dataset] is created.
Dataset [DatasetSR - test_dataset] is created.
Pass this initialization! Initialization was done during network definition!
Pass this initialization! Initialization was done during network definition!
Training model [ModelPlain] is created.
Loading model for G [SwinITMO/ITMO TRAINING/models/100_G.pth] ...
Loading model for E [SwinITMO/ITMO TRAINING/models/100_E.pth] ...
Loading optimizerG [SwinITMO/ITMO TRAINING/models/100_optimizerG.pth] ...
/home/slee67/ENV/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/slee67/ENV/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/home/slee67/ENV/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:429: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
23-04-18 04:55:17.182 : <epoch:  0, iter:     101, lr:2.000e-04> G_loss: 8.356e-02 
23-04-18 04:55:18.041 : <epoch:  0, iter:     102, lr:2.000e-04> G_loss: 7.283e-02 
23-04-18 04:55:18.869 : <epoch:  0, iter:     103, lr:2.000e-04> G_loss: 1.292e-01 
23-04-18 04:55:19.918 : <epoch:  0, iter:     104, lr:2.000e-04> G_loss: 8.281e-02 
23-04-18 04:55:21.207 : <epoch:  0, iter:     105, lr:2.000e-04> G_loss: 6.493e-02 
23-04-18 04:55:22.777 : <epoch:  0, iter:     106, lr:2.000e-04> G_loss: 6.804e-02 
23-04-18 04:55:24.439 : <epoch:  0, iter:     107, lr:2.000e-04> G_loss: 1.118e-01 
23-04-18 04:55:26.387 : <epoch:  0, iter:     108, lr:2.000e-04> G_loss: 1.422e-01 
23-04-18 04:55:28.463 : <epoch:  0, iter:     109, lr:2.000e-04> G_loss: 9.118e-02 
23-04-18 04:55:31.411 : <epoch:  0, iter:     110, lr:2.000e-04> G_loss: 1.278e-01 
23-04-18 04:55:33.862 : <epoch:  0, iter:     111, lr:2.000e-04> G_loss: 1.211e-01 
23-04-18 04:55:36.546 : <epoch:  0, iter:     112, lr:2.000e-04> G_loss: 1.546e-01 
23-04-18 04:55:39.577 : <epoch:  0, iter:     113, lr:2.000e-04> G_loss: 1.102e-01 
23-04-18 04:55:42.413 : <epoch:  0, iter:     114, lr:2.000e-04> G_loss: 1.048e-01 
23-04-18 04:55:46.985 : <epoch:  0, iter:     115, lr:2.000e-04> G_loss: 8.652e-02 
23-04-18 04:55:50.862 : <epoch:  0, iter:     116, lr:2.000e-04> G_loss: 1.060e-01 
23-04-18 04:55:54.301 : <epoch:  0, iter:     117, lr:2.000e-04> G_loss: 9.560e-02 
23-04-18 04:55:58.418 : <epoch:  0, iter:     118, lr:2.000e-04> G_loss: 1.413e-01 
23-04-18 04:56:02.377 : <epoch:  0, iter:     119, lr:2.000e-04> G_loss: 1.336e-01 
23-04-18 04:56:06.563 : <epoch:  0, iter:     120, lr:2.000e-04> G_loss: 8.754e-02 
23-04-18 04:56:10.136 : <epoch:  0, iter:     121, lr:2.000e-04> G_loss: 7.303e-02 
23-04-18 04:56:13.629 : <epoch:  0, iter:     122, lr:2.000e-04> G_loss: 1.265e-01 
23-04-18 04:56:17.583 : <epoch:  0, iter:     123, lr:2.000e-04> G_loss: 1.218e-01 
23-04-18 04:56:21.906 : <epoch:  0, iter:     124, lr:2.000e-04> G_loss: 7.314e-02 
23-04-18 04:56:25.642 : <epoch:  0, iter:     125, lr:2.000e-04> G_loss: 7.633e-02 
23-04-18 04:56:29.232 : <epoch:  0, iter:     126, lr:2.000e-04> G_loss: 7.704e-02 
23-04-18 04:56:33.161 : <epoch:  0, iter:     127, lr:2.000e-04> G_loss: 1.237e-01 
23-04-18 04:56:36.762 : <epoch:  0, iter:     128, lr:2.000e-04> G_loss: 9.011e-02 
23-04-18 04:56:41.218 : <epoch:  0, iter:     129, lr:2.000e-04> G_loss: 1.386e-01 
23-04-18 04:56:44.838 : <epoch:  0, iter:     130, lr:2.000e-04> G_loss: 7.939e-02 
23-04-18 04:56:48.758 : <epoch:  0, iter:     131, lr:2.000e-04> G_loss: 8.756e-02 
23-04-18 04:56:52.266 : <epoch:  0, iter:     132, lr:2.000e-04> G_loss: 8.264e-02 
23-04-18 04:56:56.150 : <epoch:  0, iter:     133, lr:2.000e-04> G_loss: 5.708e-02 
23-04-18 04:57:00.914 : <epoch:  0, iter:     134, lr:2.000e-04> G_loss: 7.499e-02 
23-04-18 04:57:04.627 : <epoch:  0, iter:     135, lr:2.000e-04> G_loss: 1.120e-01 
23-04-18 04:57:08.071 : <epoch:  0, iter:     136, lr:2.000e-04> G_loss: 6.518e-02 
23-04-18 04:57:11.770 : <epoch:  0, iter:     137, lr:2.000e-04> G_loss: 5.751e-02 
23-04-18 04:57:15.542 : <epoch:  0, iter:     138, lr:2.000e-04> G_loss: 7.502e-02 
23-04-18 04:57:19.952 : <epoch:  0, iter:     139, lr:2.000e-04> G_loss: 6.560e-02 
23-04-18 04:57:23.861 : <epoch:  0, iter:     140, lr:2.000e-04> G_loss: 5.679e-02 
23-04-18 04:57:27.391 : <epoch:  0, iter:     141, lr:2.000e-04> G_loss: 8.568e-02 
23-04-18 04:57:30.972 : <epoch:  0, iter:     142, lr:2.000e-04> G_loss: 1.003e-01 
23-04-18 04:57:35.634 : <epoch:  0, iter:     143, lr:2.000e-04> G_loss: 9.826e-02 
23-04-18 04:57:39.245 : <epoch:  0, iter:     144, lr:2.000e-04> G_loss: 5.908e-02 
23-04-18 04:57:43.304 : <epoch:  0, iter:     145, lr:2.000e-04> G_loss: 5.295e-02 
23-04-18 04:57:47.084 : <epoch:  0, iter:     146, lr:2.000e-04> G_loss: 1.022e-01 
23-04-18 04:57:50.938 : <epoch:  0, iter:     147, lr:2.000e-04> G_loss: 9.802e-02 
23-04-18 04:57:55.306 : <epoch:  0, iter:     148, lr:2.000e-04> G_loss: 4.949e-02 
23-04-18 04:57:59.073 : <epoch:  0, iter:     149, lr:2.000e-04> G_loss: 7.076e-02 
23-04-18 04:58:02.865 : <epoch:  0, iter:     150, lr:2.000e-04> G_loss: 6.911e-02 
23-04-18 04:58:06.660 : <epoch:  0, iter:     151, lr:2.000e-04> G_loss: 8.214e-02 
23-04-18 04:58:10.315 : <epoch:  0, iter:     152, lr:2.000e-04> G_loss: 6.953e-02 
23-04-18 04:58:14.757 : <epoch:  0, iter:     153, lr:2.000e-04> G_loss: 6.137e-02 
23-04-18 04:58:18.540 : <epoch:  0, iter:     154, lr:2.000e-04> G_loss: 6.513e-02 
23-04-18 04:58:22.463 : <epoch:  0, iter:     155, lr:2.000e-04> G_loss: 9.999e-02 
23-04-18 04:58:26.011 : <epoch:  0, iter:     156, lr:2.000e-04> G_loss: 7.974e-02 
23-04-18 04:58:30.498 : <epoch:  0, iter:     157, lr:2.000e-04> G_loss: 7.864e-02 
23-04-18 04:58:34.071 : <epoch:  0, iter:     158, lr:2.000e-04> G_loss: 4.184e-02 
23-04-18 04:58:37.874 : <epoch:  0, iter:     159, lr:2.000e-04> G_loss: 9.672e-02 
23-04-18 04:58:41.598 : <epoch:  0, iter:     160, lr:2.000e-04> G_loss: 9.528e-02 
23-04-18 04:58:45.400 : <epoch:  0, iter:     161, lr:2.000e-04> G_loss: 7.533e-02 
23-04-18 04:58:49.989 : <epoch:  0, iter:     162, lr:2.000e-04> G_loss: 6.730e-02 
23-04-18 04:58:53.661 : <epoch:  0, iter:     163, lr:2.000e-04> G_loss: 7.322e-02 
23-04-18 04:58:57.231 : <epoch:  0, iter:     164, lr:2.000e-04> G_loss: 7.112e-02 
23-04-18 04:59:01.049 : <epoch:  0, iter:     165, lr:2.000e-04> G_loss: 1.081e-01 
23-04-18 04:59:04.675 : <epoch:  0, iter:     166, lr:2.000e-04> G_loss: 8.247e-02 
23-04-18 04:59:09.489 : <epoch:  0, iter:     167, lr:2.000e-04> G_loss: 7.394e-02 
23-04-18 04:59:13.061 : <epoch:  0, iter:     168, lr:2.000e-04> G_loss: 8.284e-02 
23-04-18 04:59:16.787 : <epoch:  0, iter:     169, lr:2.000e-04> G_loss: 4.944e-02 
23-04-18 04:59:20.633 : <epoch:  0, iter:     170, lr:2.000e-04> G_loss: 3.425e-02 
23-04-18 04:59:24.422 : <epoch:  0, iter:     171, lr:2.000e-04> G_loss: 6.294e-02 
23-04-18 04:59:28.921 : <epoch:  0, iter:     172, lr:2.000e-04> G_loss: 9.008e-02 
23-04-18 04:59:32.680 : <epoch:  0, iter:     173, lr:2.000e-04> G_loss: 1.105e-01 
23-04-18 04:59:36.524 : <epoch:  0, iter:     174, lr:2.000e-04> G_loss: 9.356e-02 
23-04-18 04:59:40.058 : <epoch:  0, iter:     175, lr:2.000e-04> G_loss: 4.412e-02 
23-04-18 04:59:44.043 : <epoch:  0, iter:     176, lr:2.000e-04> G_loss: 1.249e-01 
23-04-18 04:59:47.812 : <epoch:  0, iter:     177, lr:2.000e-04> G_loss: 9.502e-02 
23-04-18 04:59:51.558 : <epoch:  0, iter:     178, lr:2.000e-04> G_loss: 7.364e-02 
23-04-18 04:59:55.236 : <epoch:  0, iter:     179, lr:2.000e-04> G_loss: 5.704e-02 
23-04-18 04:59:59.253 : <epoch:  0, iter:     180, lr:2.000e-04> G_loss: 8.612e-02 
23-04-18 05:00:04.081 : <epoch:  0, iter:     181, lr:2.000e-04> G_loss: 8.281e-02 
23-04-18 05:00:07.467 : <epoch:  0, iter:     182, lr:2.000e-04> G_loss: 8.821e-02 
23-04-18 05:00:11.168 : <epoch:  0, iter:     183, lr:2.000e-04> G_loss: 9.097e-02 
23-04-18 05:00:14.898 : <epoch:  0, iter:     184, lr:2.000e-04> G_loss: 7.349e-02 
23-04-18 05:00:18.989 : <epoch:  0, iter:     185, lr:2.000e-04> G_loss: 7.699e-02 
23-04-18 05:00:23.592 : <epoch:  0, iter:     186, lr:2.000e-04> G_loss: 6.297e-02 
23-04-18 05:00:27.250 : <epoch:  0, iter:     187, lr:2.000e-04> G_loss: 4.240e-02 
23-04-18 05:00:31.093 : <epoch:  0, iter:     188, lr:2.000e-04> G_loss: 3.106e-02 
23-04-18 05:00:35.210 : <epoch:  0, iter:     189, lr:2.000e-04> G_loss: 1.042e-01 
23-04-18 05:00:38.722 : <epoch:  0, iter:     190, lr:2.000e-04> G_loss: 1.104e-01 
23-04-18 05:00:43.366 : <epoch:  0, iter:     191, lr:2.000e-04> G_loss: 1.018e-01 
23-04-18 05:00:47.531 : <epoch:  0, iter:     192, lr:2.000e-04> G_loss: 5.077e-02 
23-04-18 05:00:51.094 : <epoch:  0, iter:     193, lr:2.000e-04> G_loss: 1.161e-01 
23-04-18 05:00:54.783 : <epoch:  0, iter:     194, lr:2.000e-04> G_loss: 4.611e-02 
23-04-18 05:00:59.378 : <epoch:  0, iter:     195, lr:2.000e-04> G_loss: 1.034e-01 
23-04-18 05:01:03.230 : <epoch:  0, iter:     196, lr:2.000e-04> G_loss: 7.367e-02 
23-04-18 05:01:06.746 : <epoch:  0, iter:     197, lr:2.000e-04> G_loss: 8.537e-02 
23-04-18 05:01:10.556 : <epoch:  0, iter:     198, lr:2.000e-04> G_loss: 8.434e-02 
23-04-18 05:01:24.399 : <epoch:  0, iter:     199, lr:2.000e-04> G_loss: 5.508e-02 
23-04-18 05:01:32.636 : <epoch:  0, iter:     200, lr:2.000e-04> G_loss: 8.805e-02 
23-04-18 05:01:32.636 : Saving the model.
Traceback (most recent call last):
  File "/home/slee67/KAIR/main_train_psnr.py", line 253, in <module>
    main()
  File "/home/slee67/KAIR/main_train_psnr.py", line 226, in main
    model.test()
  File "/home/slee67/KAIR/models/model_plain.py", line 201, in test
    self.netG_forward()
  File "/home/slee67/KAIR/models/model_plain.py", line 158, in netG_forward
    self.E = self.netG(self.L)
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 171, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 181, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py", line 89, in parallel_apply
    output.reraise()
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/_utils.py", line 543, in reraise
    raise exception
torch.cuda.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py", line 64, in _worker
    output = module(*input, **kwargs)
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/slee67/KAIR/models/network_swinir.py", line 834, in forward
    res = self.conv_after_body(self.forward_features(x_first)) + x_first
  File "/home/slee67/KAIR/models/network_swinir.py", line 798, in forward_features
    x = layer(x, x_size)
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/slee67/KAIR/models/network_swinir.py", line 482, in forward
    return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))) + x
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/slee67/KAIR/models/network_swinir.py", line 402, in forward
    x = blk(x, x_size)
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/slee67/KAIR/models/network_swinir.py", line 262, in forward
    attn_windows = self.attn(x_windows, mask=self.calculate_mask(x_size).to(x.device))
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/slee67/KAIR/models/network_swinir.py", line 121, in forward
    qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/slee67/ENV/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.69 GiB (GPU 0; 39.39 GiB total capacity; 24.74 GiB already allocated; 12.21 GiB free; 24.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

